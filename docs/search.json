[{"path":"/articles/estimate_comparisons.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Estimate Comparisons","text":"’ll focus single county (Arlington, VA), census block groups Public Use Microdata Samples (PUMS) primary source data. Ultimately, ’ll want use methods get values unobserved geographies, comparison methods, ’ll want way test results known values. test case, can simply start higher geolevel, make estimates block group level. , ’ll need aggregate block group data tracts:","code":"# directory to save data in base_dir <- \"../estimate_comparison\" dir.create(base_dir, FALSE) # geographies library(catchment) geography_bg <- download_census_shapes(base_dir, \"va\", \"bg\", year = 2021) geography_bg <- geography_bg[grep(\"^51013\", geography_bg$GEOID), ] geography_tr <- download_census_shapes(base_dir, \"va\", \"tr\", year = 2021) geography_tr <- geography_tr[grep(\"^51013\", geography_tr$GEOID), ]  # ACS data  ## block groups block_groups <- tidycensus::get_acs(   year = 2021,   state = \"51\",   county = \"013\",   geography = \"block group\",   output = \"wide\",   variables = c(     total = \"B01001_001\",     race_white = \"B02001_002\",     race_black = \"B02001_003\",     race_native = \"B02001_004\",     race_asian = \"B02001_005\",     sex_male = \"B01001_002\",     sex_female = \"B01001_026\",     tenure_total = \"B25003_001\",     tenure_owner = \"B25003_002\",     tenure_renter = \"B25003_003\",     income_total = \"B19001_001\",     income_lt10 = \"B19001_002\",     income_10_15 = \"B19001_003\",     income_15_20 = \"B19001_004\",     income_20_25 = \"B19001_005\",     income_25_30 = \"B19001_006\",     income_30_35 = \"B19001_007\",     income_35_40 = \"B19001_008\",     income_40_45 = \"B19001_009\",     income_45_50 = \"B19001_010\",     income_50_60 = \"B19001_011\",     income_60_75 = \"B19001_012\",     income_75_100 = \"B19001_013\",     income_100_125 = \"B19001_014\",     income_125_150 = \"B19001_015\",     income_150_200 = \"B19001_016\",     income_gt200 = \"B19001_017\",     avg_hhsize = \"B25010_001\",     avg_hhsize_own = \"B25010_002\",     avg_hhsize_rent = \"B25010_003\"   ) )[, -2] #> Getting data from the 2017-2021 5-year ACS colnames(block_groups)[2] <- \"Total\" block_groups$race_other <- block_groups$Total - rowSums(   block_groups[, grep(\"^race_.*E$\", colnames(block_groups))] ) colnames(block_groups) <- sub(\"E$\", \"\", colnames(block_groups))  ### impute missing average household sizes block_groups$avg_hhsize[is.na(block_groups$avg_hhsize)] <- mean(   block_groups$avg_hhsize,   na.rm = TRUE ) su <- is.na(block_groups$avg_hhsize_own) block_groups$avg_hhsize_own[su] <- rowMeans(   block_groups[su, c(\"avg_hhsize\", \"avg_hhsize_rent\")],   na.rm = TRUE ) block_groups[is.na(block_groups)] <- 0  ### make larger income groups income_vars <- grep(\"income\", colnames(block_groups), value = TRUE) block_groups$income_lt50 <- rowSums(   block_groups[, grep(\"_(?:lt1|[1-4][05]_).*\\\\d$\", income_vars, value = TRUE)] ) block_groups$income_50_100 <- rowSums(   block_groups[, grep(\"income_[5-7].*\\\\d$\", income_vars, value = TRUE)] ) block_groups$income_100_200 <- rowSums(   block_groups[, grep(\"_1\\\\d{2}.*\\\\d$\", income_vars, value = TRUE)] ) income_vars_select <- c(   \"income_lt50\", \"income_50_100\", \"income_100_200\", \"income_gt200\" )  library(sf) rownames(geography_bg) <- geography_bg$GEOID st_geometry(block_groups) <- st_geometry(geography_bg[block_groups$GEOID, ])  # parcel data parcel_file <- paste0(base_dir, \"/parcels.rds\") if (!file.exists(parcel_file)) {   parcels <- st_read(paste0(     \"https://arlgis.arlingtonva.us/arcgis/rest/\",     \"services/Open_Data/od_MHUD_Polygons/\",     \"FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=json\"   ))   saveRDS(parcels, parcel_file, compress = \"xz\") } parcels <- readRDS(parcel_file) colnames(parcels)[colnames(parcels) == \"Total_Units\"] <- \"Total\" parcels$OBJECTID <- as.character(parcels$OBJECTID) parcels <- parcels[parcels$Year_Built <= 2021, ] tracts <- redistribute(block_groups, geography_tr)"},{"path":"/articles/estimate_comparisons.html","id":"redistribution","dir":"Articles","previous_headings":"","what":"Redistribution","title":"Estimate Comparisons","text":"baseline, first method consider proportional redistribution block group summary estimates alone.","code":""},{"path":"/articles/estimate_comparisons.html","id":"direct","dir":"Articles","previous_headings":"Redistribution","what":"Direct","title":"Estimate Comparisons","text":"tracts block groups directly: can see case, error introduce proportional information lower level; block groups varied within tract now value:","code":"# going from tracts to block groups with no information estimate_tr_to_bg <- redistribute(   tracts, block_groups,   source_id = \"id\" )  # mean absolute error between total population estimates mean(abs(estimate_tr_to_bg$Total - block_groups$Total)) #> [1] 319.8405"},{"path":"/articles/estimate_comparisons.html","id":"down-and-up","dir":"Articles","previous_headings":"Redistribution","what":"Down and Up","title":"Estimate Comparisons","text":"go tracts parcels, back block groups: Now original variation block groups:  parcel totals living units rather individuals, may able refine redistribution based heuristic estimates number people per living unit: better sense resident adjustment , can look difference made single tract: Now can look difference contained parcels block groups:  case, difference mostly comes two buildings many units get less relative weight resident values.","code":"# function to apply tract to parcel to block group redistribution redist_downup <- function(     top, bottom, middle, map_tb = NULL, map_bm = NULL,     weight = \"Total\", source_id = \"id\",     target_id = \"OBJECTID\", target_id_mid = \"GEOID\") {   to_bottom <- redistribute(     top, bottom, map_tb,     weight = weight, source_id = source_id, target_id = target_id,     default_value = 0   )   redistribute(     to_bottom, middle, map_bm,     source_id = \"id\", target_id = target_id_mid,     default_value = 0   ) }  # pre-make maps  ## from block groups to parcels map_bg_to_parcel <- redistribute(   block_groups, parcels,   target_id = \"OBJECTID\", return_map = TRUE, overlaps = \"first\" ) parcels$geoid <- structure(   rep(names(map_bg_to_parcel), vapply(map_bg_to_parcel, length, 0)),   names = names(unlist(unname(map_bg_to_parcel))) )[parcels$OBJECTID] block_groups <- block_groups[block_groups$GEOID %in% parcels$geoid, ]  ## from tracts to parcels map_tr_to_parcel <- split(unlist(unname(map_bg_to_parcel)), rep(   substring(names(map_bg_to_parcel), 1, 11), vapply(map_bg_to_parcel, length, 0) ))  # check that down from and up to the same level works all.equal(   as.data.frame(redist_downup(     block_groups, parcels, block_groups, map_bg_to_parcel, map_bg_to_parcel,     source_id = \"GEOID\"   )[, -1]),   as.data.frame(block_groups[, -1]) ) #> [1] TRUE  # redistribute estimate_redist <- redist_downup(   tracts, parcels, block_groups, map_tr_to_parcel, map_bg_to_parcel )  # mean absolute error between total population estimates mean(abs(estimate_redist$Total - block_groups$Total)) #> [1] 231.1852 # 2.5 for multi-unit parcels, 5.5 for single-unit parcels parcels$Residents <- parcels$Total * (2.5 + (parcels$Total == 1) * 3)  # now see if this improved our redistribution estimate_redist_adj <- redist_downup(   tracts, parcels, block_groups, map_tr_to_parcel, map_bg_to_parcel,   weight = \"Residents\" )  # mean absolute error between total population estimates mean(abs(estimate_redist_adj$Total - block_groups$Total)) #> [1] 194.0749 # identify the most different tract between variants tract <- substring(block_groups$GEOID[   which.max(abs(estimate_redist_adj$Total - estimate_redist$Total)) ], 1, 11) su <- substring(estimate_redist$id, 1, 11) == tract tsu <- tracts$id == tract psu <- substring(parcels$geoid, 1, 11) == tract  # recalculate parcel-level values within that tract differences <- list(   parcels = redistribute(     tracts$Total[tsu], parcels[psu, ],     target_id = \"OBJECTID\", weight = \"Residents\", return_geometry = FALSE   )[, 2] - redistribute(     tracts$Total[tsu], parcels[psu, ],     target_id = \"OBJECTID\", weight = \"Total\", return_geometry = FALSE   )[, 2],   block_group = estimate_redist_adj$Total[su] - estimate_redist$Total[su] )"},{"path":"/articles/estimate_comparisons.html","id":"pums-raking","dir":"Articles","previous_headings":"","what":"PUMS + Raking","title":"Estimate Comparisons","text":"far, ’ve used additional information (total units) parcel level improve distribution values across block groups, relative uniform distribution. information parcel level, unit type number renter versus owner designations, make use information, need association source values features. census microdata sample can come : Public Use Microdata Sample (PUMS) consists individual-level observations include demographic features housing features. PUM sample located PUM areas, made tracts, can start baseline. use associations demographic housing features within PUMA get probabilities parcel level, redistribute source values like . first step end prepare data easy associate individual PUMS variables per-level summaries tract level: Now can apply method set data. new methods, can also update heuristic adjustment proportional redistribution method PUMS information:","code":"pums <- download_census_pums(base_dir, \"va\", 2021, one_year = FALSE) #> ℹ loading household sample: h.csv.xz #> ℹ loading person sample: p.csv.xz #> ℹ loading crosswalk 2010_Census_Tract_to_2010_PUMA.txt  # prepare IDs pums$crosswalk$GEOID <- do.call(paste0, pums$crosswalk[, 1:3]) pums$person$ID <- do.call(paste0, pums$person[, c(\"SERIALNO\", \"SPORDER\")])  # prepare variables  ## survey categories pums$person$sex_cat <- c(\"sex_male\", \"sex_female\")[as.numeric(pums$person$SEX)]  pums$person$race_cat <- paste0(\"race_\", c(   \"white\", \"black\", \"native\", rep(\"other\", 2), \"asian\", rep(\"other\", 3) ))[as.numeric(pums$person$RAC1P)]  pums$household$income_cat <- as.character(cut(   pums$household$HINCP, c(-Inf, 50, 100, 200, Inf) * 1e3, income_vars_select,   right = FALSE ))  ## unit categories pums$household <- pums$household[   !is.na(pums$household$BLD) & !is.na(pums$household$TEN), ] pums$household$building_type <- \"MULTI\" pums$household$building_type[pums$household$BLD == \"02\"] <- \"SFD\" pums$household$building_type[pums$household$BLD == \"03\"] <- \"SFA\"  pums$household$status <- \"RENTER\" pums$household$status[pums$household$TEN %in% 1:2] <- \"OWNER\" # define variables of interest vars_house <- c(   ID = \"SERIALNO\", weight = \"WGTP\", type = \"building_type\",   status = \"status\", income = \"income_cat\" ) vars_person <- c(   ID = \"ID\", weight = \"PWGTP\", sex_cat = \"sex_cat\", race_cat = \"race_cat\" ) vars_units <- c(   type = \"Unit_Type\", renters = \"Renter_Occupied\", owners = \"Owner_Occupied\" )  ## get their levels vars_list <- c(   lapply(vars_person[-(1:2)], function(var) unique(pums$person[[var]])),   income_cat = list(unique(pums$household$income_cat)) ) return_vars <- names(vars_list) pvars <- unlist(vars_list, use.names = FALSE) vars <- c(pvars, \"avg_hhsize_own\", \"avg_hhsize_rent\")  # prepare datasets split into PUMAs pumas_focal <- unique(pums$crosswalk$PUMA5CE[pums$crosswalk$GEOID %in% tracts$id]) data <- lapply(structure(pumas_focal, names = pumas_focal), function(puma) {   households <- as.data.frame(pums$household[pums$household$PUMA == puma, vars_house])   rownames(households) <- households$SERIALNO   ids <- pums$crosswalk$GEOID[     pums$crosswalk$PUMA5CE == puma & pums$crosswalk$GEOID %in% tracts$id   ]   person <- pums$person[pums$person$SERIALNO %in% households$SERIALNO, ]   list(     individuals = cbind(households[person$SERIALNO, ], person),     source = tracts[tracts$id %in% ids, c(\"id\", vars), drop = TRUE],     parcels = parcels[       substring(parcels$geoid, 1, 11) %in% ids, c(\"geoid\", \"OBJECTID\", vars_units),       drop = TRUE     ]   ) })  # function to apply each method to the data apply_method <- function(data, method, parcel_only = FALSE, rescale = TRUE) {   p <- do.call(rbind, lapply(data, function(set) {     set$source <- st_drop_geometry(set$source)     do.call(rbind, lapply(unique(set$source$id), function(id) {       source <- set$source[set$source$id == id, -1]       target <- set$parcels[if (\"map\" %in% names(set)) {         if (\"geography\" %in% names(set)) {           set$parcels$geoid %in% set$map[[id]]         } else {           set$parcels$geoid == id         }       } else {         substring(set$parcels$geoid, 1, 11) == id       }, ]       if (nrow(target)) {         est <- if (sum(source)) method(source, target[, -1], set$individuals) else NULL         if (length(est)) {           if (rescale) {             totals <- colSums(est)             totals[!totals] <- 1             est <- sweep(est, 2, totals, \"/\") * rep(               as.numeric(source[, colnames(est)]),               each = nrow(est)             )           }           su <- !pvars %in% colnames(est)           if (any(su)) est[, pvars[su]] <- 0           cbind(target[, 1:2], est[as.character(target$OBJECTID), pvars])         } else {           est <- target[, 1:2]           est[, pvars] <- 0           est         }       }     }))   }))   if (parcel_only) {     p   } else {     list(parcels = p, block_groups = redistribute(       p[, -1], block_groups, map_bg_to_parcel,       source_id = \"OBJECTID\", target_id = \"GEOID\", default_value = 0     ))   } }  # function to calculate estimates from weights make_estimates <- function() {   do.call(rbind, lapply(unique(target$Unit_Type), function(type) {     d <- target[       target$Unit_Type == type, c(\"OBJECTID\", \"Renter_Occupied\", \"Owner_Occupied\")     ]     d$Renter_Occupied <- d$Renter_Occupied * source$avg_hhsize_rent     d$Owner_Occupied <- d$Owner_Occupied * source$avg_hhsize_own     nd <- nrow(d)     colnames(d)[-1] <- c(\"RENTER\", \"OWNER\")     i <- individuals[       individuals$building_type == type, c(\"weight\", \"status\", return_vars)     ]     as.data.frame(Reduce(\"+\", lapply(       (if (is.factor(i$status)) levels else unique)(i$status),       function(s) {         ii <- i[i$status == s, ]         weight_total <- sum(ii$weight)         do.call(cbind, lapply(return_vars, function(cat) {           props <- tapply(ii$weight, ii[[cat]], sum) / weight_total           props[vars_list[[cat]][!vars_list[[cat]] %in% names(props)]] <- 0           props[is.na(props)] <- 0           props <- props[vars_list[[cat]]]           matrix(             d[[s]] * rep(props, each = nd),             nd,             dimnames = list(d$OBJECTID, names(props))           )         }))       }     )))   })) } # make single-family-home indicators pums$household$single_family_home <- pums$household$building_type != \"MULTI\" parcels$single_family_home <- parcels$Unit_Type != \"MULTI\"  # update resident estimates with average household size for # single family versus other home types within the county pums$household$size <- table(pums$person$SERIALNO)[pums$household$SERIALNO] households <- pums$household[pums$household$PUMA %in% pumas_focal, ] parcels$Residents <- parcels$Total * tapply(   households$size, households$single_family_home, mean )[as.character(parcels$single_family_home)]  # recalculate the adjusted redistribution results estimate_redist_adj <- redist_downup(   tracts, parcels, block_groups, map_tr_to_parcel, map_bg_to_parcel,   weight = \"Residents\" ) mean(abs(estimate_redist_adj$Total - block_groups$Total)) #> [1] 201.2044"},{"path":"/articles/estimate_comparisons.html","id":"initial-weights","dir":"Articles","previous_headings":"PUMS + Raking","what":"Initial Weights","title":"Estimate Comparisons","text":"baseline, can calculate set proportions PUMA, apply parcels contained tract: method make estimates recover initial values, rescale comparable. illustration:","code":"# function to apply the method: # - source: the tract-level totals # - target: the parcels # - individuals: the individual PUM responses est_baseline <- function(source, target, individuals) {   individuals$weight <- individuals$PWGTP   environment(make_estimates) <- environment()   make_estimates() }  # apply across PUMAs and tracts within them # and aggregate to block groups estimate_baseline <- apply_method(data, est_baseline) # adjust tract counts so they match # unit totals * average household size across parcels tr_adj <- st_drop_geometry(tracts) pid <- substring(parcels$geoid, 1, 11) parcels$renter_est <- parcels$Renter_Occupied * structure(   tracts$avg_hhsize_rent,   names = tracts$id )[pid] parcels$owner_est <- parcels$Owner_Occupied * structure(   tracts$avg_hhsize_own,   names = tracts$id )[pid] parcels$resident_est <- parcels$renter_est + parcels$owner_est tr_adj$Total <- tapply(parcels$resident_est, pid, sum)[tr_adj$id]  for (g in vars_list) {   tr_adj[, g] <- tr_adj[, g] / rowSums(tr_adj[, g]) * tr_adj$Total } tr_adj[is.na(tr_adj)] <- 0  # make baseline parcel-level estimates without rescaling data_adj <- data data_adj[[1]]$source <- tr_adj[tr_adj$id %in% data[[1]]$source$id, c(\"id\", vars)] data_adj[[2]]$source <- tr_adj[tr_adj$id %in% data[[2]]$source$id, c(\"id\", vars)] parcel_adj <- apply_method(   data_adj, est_baseline,   rescale = FALSE, parcel_only = TRUE )  # aggregate back up to tracts and measure per-category error tr_est <- redistribute(   parcel_adj, tr_adj, map_tr_to_parcel,   source_id = \"OBJECTID\", target_id = \"id\", default_value = 0 ) data.frame(MAE = round(vapply(   pvars, function(var) mean(abs(tr_est[[var]] - tr_adj[[var]])), 0 ), 5)) #>                       MAE #> sex_female     1401.79870 #> sex_male       1491.87143 #> race_black      627.37675 #> race_white     2384.51148 #> race_other      805.31467 #> race_asian      630.67565 #> race_native      47.13013 #> income_lt50    1037.31028 #> income_gt200   1082.48155 #> income_100_200 1558.09126 #> income_50_100  1051.30114  ## this method does recover the parcel-level totals and overall totals g <- c(\"sex_female\", \"sex_male\") all(abs(rowSums(parcel_adj[, g]) - structure(   parcels$resident_est,   names = parcels$OBJECTID )[parcel_adj$OBJECTID]) < 1e-12) #> [1] TRUE c(   original = sum(     rbind(data_adj[[1]]$source[, g], data_adj[[2]]$source[, g])   ),   parcel = sum(parcel_adj[, g]),   tract = sum(tr_est[, g]) ) #> original   parcel    tract  #> 691326.4 691326.4 691326.4  # rescaling makes the totals line up parcel_adj_rescaled <- apply_method(   data_adj, est_baseline,   parcel_only = TRUE ) tr_est_rescaled <- redistribute(   parcel_adj_rescaled, tr_adj, map_tr_to_parcel,   source_id = \"OBJECTID\", target_id = \"id\", default_value = 0 ) data.frame(MAE = round(vapply(   pvars, function(var) mean(abs(tr_est_rescaled[[var]] - tr_adj[[var]])), 0 ), 5)) #>                       MAE #> sex_female     1187.18086 #> sex_male       1277.25358 #> race_black      222.61277 #> race_white     1614.08909 #> race_other      313.43071 #> race_asian      298.95131 #> race_native      15.35056 #> income_lt50     445.50091 #> income_gt200    542.19950 #> income_100_200  921.30049 #> income_50_100   555.43353  # just like the redistribution methods tr_redist <- redist_downup(   tr_adj, parcels, tr_adj, map_tr_to_parcel, map_tr_to_parcel ) data.frame(MAE = round(vapply(   pvars, function(var) mean(abs(tr_redist[[var]] - tr_adj[[var]])), 0 ), 5)) #>                MAE #> sex_female       0 #> sex_male         0 #> race_black       0 #> race_white       0 #> race_other       0 #> race_asian       0 #> race_native      0 #> income_lt50      0 #> income_gt200     0 #> income_100_200   0 #> income_50_100    0"},{"path":"/articles/estimate_comparisons.html","id":"raking","dir":"Articles","previous_headings":"PUMS + Raking","what":"Raking","title":"Estimate Comparisons","text":"baseline, ’re using PUMS weights calculate proportions, weights meant bring totals line PUMA-level estimates; Since lower-level information form tract-level totals, can adjust weights match smaller-area summary:","code":"library(anesrake) rake_prep <- function() {   eval(expression({     for (var in names(totals)) {       individuals[[var]] <- as.factor(individuals[[var]])       l <- levels(individuals[[var]])       totals[[var]] <- totals[[var]][l]       su <- is.na(totals[[var]]) | totals[[var]] == 0       if (sum(su)) {         individuals <- individuals[!individuals[[var]] %in% l[su], ]         individuals[[var]] <- droplevels(individuals[[var]])         totals[[var]] <- totals[[var]][!su]       }       total <- sum(totals[[var]])       if (total) totals[[var]] <- totals[[var]] / total     }     totals <- Filter(length, totals)   }), parent.frame()) } est_rake <- function(source, target, individuals) {   totals <- lapply(     structure(names(vars_list)[1:2], names = names(vars_list)[1:2]),     function(n) unlist(source[, vars_list[[n]]])   )   rake_prep()   individuals$status <- as.factor(individuals$status)   individuals$building_type <- as.factor(individuals$building_type)   individuals$income_cat <- as.factor(individuals$income_cat)   capture.output(individuals$weight <- tryCatch(     anesrake(       totals, individuals, individuals$ID, individuals$PWGTP,       pctlim = .001     )$weightvec[as.character(individuals$ID)],     error = function(e) {       warning(e$message)       individuals$PWGTP     }   ))   environment(make_estimates) <- environment()   make_estimates() } estimate_rake <- apply_method(data, est_rake)"},{"path":"/articles/estimate_comparisons.html","id":"two-step-raking","dir":"Articles","previous_headings":"PUMS + Raking","what":"Two Step Raking","title":"Estimate Comparisons","text":"initial raking approach, considered person-level variables, also household-level information, might try account well first adjusting household weights, using adjusted weights initialize adjusted person-level weights:","code":"est_twostep <- function(source, target, individuals) {   all <- individuals   individuals <- individuals[!duplicated(individuals$SERIALNO), ]   # step 1: initial weights at household level   totals <- list(     income_cat = unlist(source[, grep(\"^income_\", colnames(source))]),     status = structure(       colSums(target[, vars_units[-1]]),       names = c(\"RENTER\", \"OWNER\")     ),     building_type = tapply(rowSums(target[, vars_units[-1]]), target$Unit_Type, sum)   )   rake_prep()   capture.output(initial <- tryCatch(     anesrake(       totals, individuals, individuals$SERIALNO, individuals$WGTP,       pctlim = .001     )$weightvec,     error = function(e) {       warning(e$message)       structure(individuals$WGTP, names = individuals$SERIALNO)     }   ))   all$weight <- initial[as.character(all$SERIALNO)]   all <- all[!is.na(all$weight), ]   individuals <- all   # step 2: adjust at person level   totals <- lapply(     structure(names(vars_list)[1:2], names = names(vars_list)[1:2]),     function(n) unlist(source[, vars_list[[n]]])   )   rake_prep()   initial <- individuals$weight   capture.output(individuals$weight <- tryCatch(     anesrake(       totals, individuals, individuals$ID,       individuals$PWGTP * initial / individuals$WGTP,       pctlim = .001     )$weightvec[as.character(individuals$ID)],     error = function(e) {       warning(e$message)       individuals$PWGTP * initial / individuals$WGTP     }   ))   environment(make_estimates) <- environment()   make_estimates() } estimate_twostep <- apply_method(data, est_twostep)"},{"path":"/articles/estimate_comparisons.html","id":"n-step-raking","dir":"Articles","previous_headings":"PUMS + Raking","what":"N Step Raking","title":"Estimate Comparisons","text":"can take two-step method repeating process try better fit household individual data:","code":"est_nstep <- function(source, target, individuals) {   individuals$weight <- individuals$PWGTP   all <- individuals   iter <- function() {     individuals <- individuals[!duplicated(individuals$SERIALNO), ]     # step 1: initial weights at household level     totals <- list(       income_cat = unlist(source[, grep(\"^income_\", colnames(source))]),       status = structure(         colSums(target[, vars_units[-1]]),         names = c(\"RENTER\", \"OWNER\")       ),       building_type = tapply(rowSums(target[, vars_units[-1]]), target$Unit_Type, sum)     )     rake_prep()     household_totals <- totals     capture.output(initial <- tryCatch(       anesrake(         totals, individuals, individuals$SERIALNO,         individuals$weight / individuals$PWGTP * individuals$WGTP,         pctlim = .001       )$weightvec,       error = function(e) {         warning(e$message)         structure(           individuals$weight / individuals$PWGTP * individuals$WGTP,           names = individuals$SERIALNO         )       }     ))     all$weight <- initial[as.character(all$SERIALNO)]     all <- all[!is.na(all$weight), ]     individuals <- all     # step 2: adjust at person level     totals <- lapply(       structure(names(vars_list)[1:2], names = names(vars_list)[1:2]),       function(n) unlist(source[, vars_list[[n]]])     )     rake_prep()     initial <- individuals$weight     capture.output(individuals$weight <- tryCatch(       anesrake(         totals, individuals, individuals$ID,         individuals$PWGTP * initial / individuals$WGTP,         pctlim = .001       )$weightvec[as.character(individuals$ID)],       error = function(e) {         warning(e$message)         individuals$PWGTP * initial / individuals$WGTP       }     ))     hh <- individuals[!duplicated(individuals$SERIALNO), ]     weight <- hh$PWGTP / hh$weight * hh$WGTP     list(       error = mean(vapply(names(household_totals), function(v) {         mean((household_totals[[v]] - tapply(weight, hh[[v]], sum) / sum(weight))^2)       }, 0)),       data = individuals     )   }   previous_error <- 0   for (i in 1:20) {     step <- iter()     individuals <- step$data     if (step$error < .001 || abs(previous_error - step$error) < .0005) break     previous_error <- step$error   }   environment(make_estimates) <- environment()   make_estimates() } estimate_nstep <- apply_method(data, est_nstep)"},{"path":"/articles/estimate_comparisons.html","id":"generalized-raking","dir":"Articles","previous_headings":"PUMS + Raking","what":"Generalized Raking","title":"Estimate Comparisons","text":"two-step approach still mainly focuses better aligning person-level weights – household totals match first step, considered second step. Alternative , might try match household- person-level weights time generalized raking approach:","code":"library(mlfit) est_grake <- function(source, target, individuals) {   totals <- list(     person = sum(individuals$PWGTP),     household = sum(individuals$WGTP[!duplicated(individuals$SERIALNO)])   )   person <- lapply(names(vars_list)[1:2], function(n) {     l <- vars_list[[n]]     count <- as.numeric(source[, l, drop = TRUE])     count[!count] <- 1     count <- count / sum(count) * totals$person     s <- data.frame(level = l, count = count)     colnames(s)[1] <- n     s   })   household <- unique(individuals$building_type)   household <- structure(numeric(length(household)), names = household)   observed_types <- tapply(     rowSums(target[, vars_units[-1]]), target$Unit_Type, sum   )   household[names(observed_types)] <- observed_types   household <- list(data.frame(     building_type = names(household), count = household   ))   household[[1]]$count[is.na(household[[1]]$count)] <- 1   household[[2]] <- data.frame(     status = c(\"RENTER\", \"OWNER\"),     count = colSums(target[, c(\"Renter_Occupied\", \"Owner_Occupied\")])   )   household[[3]] <- unlist(source[, grep(\"^income_\", colnames(source))])   household[[3]] <- data.frame(     income_cat = names(household[[3]]), count = household[[3]]   )   names(household) <- c(\"building_type\", \"status\", \"income_cat\")   household <- lapply(names(household), function(var) {     l <- household[[var]]     l <- l[l[[1]] %in% unique(individuals[, var]), ]     l[[2]][!l[[2]]] <- 1     l[[2]] <- l[[2]] / sum(l[[2]]) * totals$household     l   })   m <- ml_problem(     individuals,     field_names = special_field_names(\"SERIALNO\", \"ID\", count = \"count\"),     group_controls = household,     individual_controls = person,     prior_weights = individuals$WGTP   )   fm <- tryCatch(     suppressWarnings(ml_fit_dss(m, ginv = solve)),     error = function(e) NULL   )   if (!isTRUE(fm$success)) {     fm <- tryCatch(       suppressWarnings(ml_fit_ipu(m)),       error = function(e) NULL     )   }   individuals$weight <- if (is.null(fm$weight)) {     individuals$PWGTP   } else {     fm$weight   }   environment(make_estimates) <- environment()   make_estimates() } estimate_grake <- apply_method(data, est_grake)"},{"path":[]},{"path":"/articles/estimate_comparisons.html","id":"tract-aggregates","dir":"Articles","previous_headings":"Comparisons","what":"Tract Aggregates","title":"Estimate Comparisons","text":"Now can compare considered methods within variables used inform raking approaches: Full results presented data site: redistribution_arlington","code":"error <- do.call(rbind, lapply(structure(pvars, names = pvars), function(var) {   d <- cbind(     Prop = estimate_redist[[var]],     \"Prop Adj\" = estimate_redist_adj[[var]],     Baseline = estimate_baseline$block_groups[[var]],     Rake = estimate_rake$block_groups[[var]],     \"Rake 2 Step\" = estimate_twostep$block_groups[[var]],     \"Rake N Step\" = estimate_nstep$block_groups[[var]],     \"Rake General\" = estimate_grake$block_groups[[var]]   )   colMeans(abs(d - block_groups[[var]])) })) error <- rbind(error, Average = colMeans(error)) kable(error, digits = 3)"},{"path":"/articles/estimate_comparisons.html","id":"alternate-geolevels","dir":"Articles","previous_headings":"Comparisons","what":"Alternate Geolevels","title":"Estimate Comparisons","text":"far, ’ve used geolevel-related test-case assess methods, may effect . alternative, might abandon canonical tracts favor randomly agglutinated block groups. can also take look virtual tracts created run: Since virtual tracts randomly created, might also look results across runs sense variation:","code":"# randomly combine adjacent block groups into pairs of 2 (were possible) # apply within each PUMA apply_altgeo <- function(run = NULL, return_full = FALSE) {   vdata <- lapply(     structure(pumas_focal, names = pumas_focal),     function(puma) {       d <- data[[puma]]       su <- substring(block_groups$GEOID, 1, 11) %in% d$source$id       vtr <- make_parent_polygons(         block_groups[su, ], \"GEOID\",         n_as_min = TRUE, verbose = FALSE       )       names(vtr$new) <- names(vtr$map) <- paste0(         puma, \"_\", seq_along(vtr$new)       )       d$geography <- vtr$new       d$map <- vtr$map       d$source <- redistribute(         block_groups[su, c(\"GEOID\", vars)], vtr$new, vtr$map,         default_value = 0       )       d     }   )   vsource <- do.call(rbind, lapply(vdata, \"[[\", \"source\"))   vmap <- redistribute(     vsource, parcels,     source_id = \"id\", target_id = \"OBJECTID\", return_map = TRUE   )   vestimates <- list(     Prop = list(block_groups = redist_downup(       vsource, parcels, block_groups, vmap, map_bg_to_parcel     )),     \"Prop Adj\" = list(block_groups = redist_downup(       vsource, parcels, block_groups, vmap, map_bg_to_parcel,       weight = \"Residents\"     )),     Baseline = apply_method(vdata, est_baseline),     Rake = apply_method(vdata, est_rake),     \"Rake 2 Step\" = apply_method(vdata, est_twostep),     \"Rake N Step\" = apply_method(vdata, est_nstep),     \"Rake General\" = apply_method(vdata, est_grake)   )   verror <- do.call(rbind, lapply(     structure(pvars, names = pvars),     function(var) {       colMeans(abs(as.data.frame(         lapply(vestimates, function(d) d$block_groups[[var]]),         check.names = FALSE       ) - block_groups[[var]]))     }   ))   verror <- rbind(verror, Average = colMeans(verror))   if (return_full) {     list(       data = data, source = vsource, map = vmap,       estimates = vestimates, error = verror     )   } else {     verror   } }  estimates_altgeo <- apply_altgeo(return_full = TRUE) kable(estimates_altgeo$error, digits = 3) # since this can be quite long-running, # we'll save and reload results results_altgeo_file <- paste0(base_dir, \"/results_altgeo.rds\") if (file.exists(results_altgeo_file)) {   results_altgeo <- readRDS(results_altgeo_file) } else {   results_altgeo <- lapply(1:50, apply_altgeo)   saveRDS(results_altgeo, results_altgeo_file) }  kable(Reduce(\"+\", results_altgeo) / length(results_altgeo), digits = 3)"},{"path":"/articles/estimate_comparisons.html","id":"simulated-populations","dir":"Articles","previous_headings":"Comparisons","what":"Simulated Populations","title":"Estimate Comparisons","text":"far, ’ve used data different initial geolevels way introduce error still known target measure error . may benefit retaining properties data affect methods unknown ways, side directly measure error, testing estimates error. alternative simulate initial population, draw samples population form simulated version data available (sparse PUM samples, less sparse block group summaries). , instead go parcels back artificially unknown intermediate layers (block groups) measure error, can measure directly parcel level. First, can look block group summaries full simulated population: redistribution methods apply census-like samples full population, ’ll set function draw sample apply methods : Now can look results single sample. comparable results () simulated independently real samples (, instance, race category baselines randomized), (b) error measured parcel level, rather block group level.","code":"# simulating the whole population is pretty intensive, # so we'll just do that once, then draw samples from it population_file <- paste0(base_dir, \"/simulated_population.rds\") if (file.exists(population_file)) {   population <- readRDS(population_file) } else {   population <- generate_population(     sum(parcels$Total), parcels, \"Total\", \"OBJECTID\"   )   population$households$SERIALNO <- population$households$household   population$individuals$ID <- population$individuals$person    # make categories    ## household   population$households$building_type <- c(     \"MULTI\", \"SFD\", \"SFA\"   )[population$households$building_type]   population$households$status <- c(\"OWNER\", \"RENTER\")[     population$households$renting + 1   ]   population$households$Total <- structure(     population$regions$capacity,     names = population$regions$id   )[population$households$region]   population$households$head_income <- population$households$head_income /     max(population$households$head_income) * max(pums$household$HINCP)   population$households$income_cat <- as.character(cut(     population$households$head_income,     c(-Inf, 50, 100, 200, Inf) * 1e3,     c(\"income_lt50\", \"income_50_100\", \"income_100_200\", \"income_gt200\"),     right = TRUE   ))    ## individual   population$individuals$sex_cat <- c(\"sex_male\", \"sex_female\")[     population$individuals$sex + 1   ]   population$individuals$race_cat <- rep_len(     grep(\"^race\", pvars, value = TRUE), max(population$individuals$race) + 1   )[population$individuals$race + 1]   population$individuals$income <- population$individuals$income /     max(population$individuals$income) * max(pums$household$HINCP)   population$individuals$income_cat <- as.character(cut(     population$individuals$income,     c(-Inf, 50, 100, 200, Inf) * 1e3,     c(\"income_lt50\", \"income_50_100\", \"income_100_200\", \"income_gt200\"),     right = TRUE   ))   combined <- cbind(     population$households[population$individuals$household, ],     population$individuals   )   population$summary <- do.call(rbind, lapply(     split(combined, as.character(combined$region)),     function(d) {       household <- d[!duplicated(d$household), ]       owner <- household$status == \"OWNER\"       renter <- household$status == \"RENTER\"       cbind(         id = household$region[[1]],         Total = household$Total[[1]],         Owner_Occupied = sum(owner),         Renter_Occupied = sum(renter),         avg_hhsize = mean(household$size),         avg_hhsize_own = if (sum(owner)) mean(household$size[owner]) else 0,         avg_hhsize_rent = if (sum(renter)) mean(household$size[renter]) else 0,         as.data.frame(as.list(unlist(lapply(           names(vars_list),           function(var) as.list(table(factor(d[[var]], vars_list[[var]])))         ))))       )     }   ))    saveRDS(population, population_file, compress = \"xz\") } ## for the initial sample, we'll aim for an even ## sampling from across block groups sample_n <- ceiling(nrow(population$households) * .1 / nrow(block_groups)) map_parcel_to_bg <- structure(   rep(names(map_bg_to_parcel), vapply(map_bg_to_parcel, length, 0)),   names = names(unlist(unname(map_bg_to_parcel))) ) draw_sample <- function(run = NULL, return_full = FALSE) {   # draw households, and retrieve individuals within them   sample_hh <- lapply(map_bg_to_parcel, function(bg) {     names(if (length(bg) < sample_n) bg else sample(bg, sample_n))   })   full <- population$households[     population$households$household %in% unlist(sample_hh, use.names = FALSE),   ]   full$WGTP <- 1   full_individuals <- population$individuals[     population$individuals$household %in% full$household,   ]   full_individuals$PWGTP <- 1   individuals <- cbind(     full[as.character(full_individuals$household), c(\"region\", vars_house)],     full_individuals[, vars_person]   )   individuals$SERIALNO <- as.character(individuals$SERIALNO)   individuals$ID <- as.character(individuals$ID)   individuals$geoid <- structure(     rep(names(sample_hh), vapply(sample_hh, length, 0)),     names = unlist(sample_hh, use.names = FALSE)   )[individuals$SERIALNO]    bg <- do.call(rbind, lapply(     split(individuals, individuals$geoid),     function(d) {       hh <- d[!duplicated(d$SERIALNO), ]       hh$size <- table(d$SERIALNO)[hh$SERIALNO]       size <- tapply(hh$size, factor(hh$status, c(\"OWNER\", \"RENTER\")), mean)       size[is.na(size)] <- 0       cbind(         id = d$geoid[[1]],         as.data.frame(unlist(lapply(           names(vars_list), function(var) {             as.list(vapply(               unique(vars_list[[var]]),               function(l) sum((d[[var]] == l) * d$PWGTP), 0             ))           }         ), recursive = FALSE)),         avg_hhsize = mean(hh$size),         avg_hhsize_own = size[[\"OWNER\"]],         avg_hhsize_rent = size[[\"RENTER\"]]       )     }   ))    ps <- cbind(     population$regions,     population$summary[population$regions$id, c(\"Total\", vars_units[-1])]   )   ps$OBJECTID <- population$regions$id   ps$geoid <- map_parcel_to_bg[population$regions$id]   ps$Unit_Type <- c(\"MULTI\", \"SFD\", \"SFA\")[ps$building_type]    ps$PUMA <- structure(     pums$crosswalk$PUMA5CE,     names = pums$crosswalk$GEOID   )[substring(ps$geoid, 1, 11)]   ps <- ps[!is.na(ps$PUMA), ]   ps$single_family_home <- ps$Unit_Type != \"MULTI\"   ps$type <- do.call(     paste0, ps[, c(\"PUMA\", \"single_family_home\"), drop = TRUE]   )    vdata <- lapply(     structure(pumas_focal, names = pumas_focal),     function(puma) {       ps <- ps[ps$PUMA == puma, ]       su <- which(individuals$region %in% ps$id)       list(         map = map_bg_to_parcel,         individuals = individuals[sample(su, floor(length(su) * .3)), ],         parcels = ps,         source = bg[bg$id %in% ps$geoid, ]       )     }   )   ps <- do.call(rbind, lapply(vdata, \"[[\", \"parcels\"))   individuals <- do.call(rbind, lapply(vdata, \"[[\", \"individuals\"))   hh <- individuals[!duplicated(individuals$SERIALNO), ]   hh$single_family_home <- hh$building_type != \"MULTI\"   hh$size <- table(individuals$SERIALNO)[as.character(hh$SERIALNO)]   ps$Residents <- ps$Total * tapply(     hh$size, hh$single_family_home, mean   )[as.character(ps$single_family_home)]    vestimates <- list(     Prop = redistribute(       bg, ps, map_bg_to_parcel,       weight = \"Total\", source_id = \"id\", default_value = 0     ),     \"Prop Adj\" = redistribute(       bg, ps, map_bg_to_parcel,       weight = \"Residents\", source_id = \"id\", default_value = 0     ),     Baseline = apply_method(vdata, est_baseline, parcel_only = TRUE),     Rake = apply_method(vdata, est_rake, parcel_only = TRUE),     \"Rake 2 Step\" = apply_method(vdata, est_twostep, parcel_only = TRUE),     \"Rake N Step\" = apply_method(vdata, est_nstep, parcel_only = TRUE),     \"Rake General\" = apply_method(vdata, est_grake, parcel_only = TRUE)   )   actual <- population$summary[ps$id, ]   verror <- do.call(rbind, lapply(     structure(pvars, names = pvars),     function(var) {       colMeans(abs(as.data.frame(         lapply(vestimates, function(d) d[[var]]),         check.names = FALSE       ) - actual[[var]]))     }   ))   verror <- rbind(verror, Average = colMeans(verror))   if (return_full) {     list(       data = data, source = bg, parcels = ps, map = map_bg_to_parcel,       estimates = vestimates, error = verror     )   } else {     verror   } } estimates_sim <- draw_sample(return_full = TRUE) kable(estimates_sim$error, digits = 5)"},{"path":"/articles/household_estimates.html","id":"direct","dir":"Articles","previous_headings":"","what":"Direct","title":"Household Estimates","text":"can start directly redistributing block group values zipcodes based geometries. First, need get source block group data (American Community Survey 5-year estimates, 2017-2021): ultimate target zipcode geometries: Now can use redistribute function make zipcode-level estimates: Since total household estimates included zipcode data (though 2022), can see close estimated total got : also use zipcode-level estimates weights improve redistribution:","code":"counties <- c(\"059\", \"600\") variables <- c(   total = \"B25006_001\",   race_white = \"B25006_002\",   race_black = \"B25006_003\",   race_native = \"B25006_004\",   race_asian = \"B25006_005\",   income_total = \"B19001_001\",   income_lt10 = \"B19001_002\",   income_10_15 = \"B19001_003\",   income_15_20 = \"B19001_004\",   income_20_25 = \"B19001_005\",   income_25_30 = \"B19001_006\",   income_30_35 = \"B19001_007\",   income_35_40 = \"B19001_008\",   income_40_45 = \"B19001_009\",   income_45_50 = \"B19001_010\",   income_50_60 = \"B19001_011\",   income_60_75 = \"B19001_012\",   income_75_100 = \"B19001_013\",   income_100_125 = \"B19001_014\",   income_125_150 = \"B19001_015\",   income_150_200 = \"B19001_016\",   income_gt200 = \"B19001_017\" ) block_groups <- tidycensus::get_acs(   \"block group\", variables,   year = 2021, output = \"wide\", cache_table = TRUE,   state = \"51\", county = counties, geometry = TRUE )[, -2] #> Getting data from the 2017-2021 5-year ACS colnames(block_groups)[2] <- \"Total\" block_groups$race_other <- block_groups$Total - rowSums(   block_groups[, grep(\"^race_.*E$\", colnames(block_groups)), drop = TRUE] ) colnames(block_groups) <- sub(\"E$\", \"\", colnames(block_groups))  # make larger income groups income_vars <- grep(\"income\", colnames(block_groups), value = TRUE) block_groups$income_lt50 <- rowSums(block_groups[, grep(   \"_(?:lt1|[1-4][05]_).*\\\\d$\", income_vars,   value = TRUE ), drop = TRUE]) block_groups$income_50_100 <- rowSums(block_groups[, grep(   \"income_[5-7].*\\\\d$\", income_vars,   value = TRUE ), drop = TRUE]) block_groups$income_100_200 <- rowSums(block_groups[, grep(   \"_1\\\\d{2}.*\\\\d$\", income_vars,   value = TRUE ), drop = TRUE]) income_vars_select <- c(   \"income_lt50\", \"income_50_100\", \"income_100_200\", \"income_gt200\" ) library(sf) zipcode_file <- paste0(base_dir, \"/zipcode_va_fairfax.rds\") if (!file.exists(zipcode_file)) {   zipcodes <- st_read(paste0(     \"https://www.fairfaxcounty.gov/euclid/rest/services/IPLS/IPLSMap\",     \"/MapServer/3/query?where=1=1&outFields=*&outSR=4326&f=geojson\"   ))   saveRDS(zipcodes, zipcode_file, compress = \"xz\") } zipcodes <- readRDS(zipcode_file) library(redistribute) map_bg_to_zipcodes <- estimates_direct <- redistribute(   block_groups, zipcodes,   target_id = \"ZIPCODE\", return_map = TRUE ) estimates_direct <- redistribute(   block_groups, zipcodes, map_bg_to_zipcodes,   target_id = \"ZIPCODE\" ) # Mean Absolute Error mean(abs(estimates_direct$Total - zipcodes$HOUSEHOLDS)) #> [1] 836.1899  # Pearson's r cor(estimates_direct$Total, zipcodes$HOUSEHOLDS) #> [1] 0.9811777 estimates_direct_weighted <- redistribute(   block_groups, zipcodes, map_bg_to_zipcodes,   target_id = \"ZIPCODE\", weight = \"HOUSEHOLDS\" ) mean(abs(estimates_direct_weighted$Total - zipcodes$HOUSEHOLDS)) #> [1] 376.5102 cor(estimates_direct_weighted$Total, zipcodes$HOUSEHOLDS) #> [1] 0.9871246"},{"path":"/articles/household_estimates.html","id":"parcels","dir":"Articles","previous_headings":"","what":"Parcels","title":"Household Estimates","text":"potentially improve estimates, can use parcel-level housing unit information disaggregate block groups parcels, aggregate zip codes: First, need get parcel data housing unit counts: , can disaggregate block groups: Note block groups contain parcels despite non-zero populations, parcel-based estimates total (7842 people unaccounted parcels): Since household estimates also available parcel level, can first see close got : Despite -coverage block groups parcels, estimated number households parcel level slightly higher (also influenced difference time): Now can aggregate parcel-level zipcodes: direct case, can also see much improvement weighting household estimates might make: parcel-level household estimates differ little unit counts, makes less difference case: Without knowing anything household estimates made parcels zipcodes, interesting note aggregated parcel estimates nearly perfectly line zipcode estimates.","code":"parcel_file <- paste0(base_dir, \"/parcel_va_fairfax.rds\") if (!file.exists(parcel_file)) {   parcels <- st_read(paste0(     \"https://opendata.arcgis.com/api/v3/datasets/\",     \"4f00b13df5a24cc19068bf356d3d1c45_1/downloads/data\",     \"?format=geojson&spatialRefId=4326\"   ))   saveRDS(parcels, parcel_file, compress = \"xz\") } parcels <- readRDS(parcel_file)  colnames(parcels)[colnames(parcels) == \"CURRE_UNIT\"] <- \"Total\" parcels$Unit_Type <- \"MULTI\" parcels$Unit_Type[parcels$HOUSI_UNIT_TYPE == \"SF\"] <- \"SFD\" parcels$Unit_Type[parcels$HOUSI_UNIT_TYPE %in% c(\"TH\", \"MP\", \"DX\")] <- \"SFA\"  parcels$PIN <- as.character(parcels$PIN) parcels <- parcels[parcels$YEAR_BUILT <= 2021 & !duplicated(parcels$PIN), ] map_bg_to_parcel <- redistribute(   block_groups, parcels,   target_id = \"PIN\", return_map = TRUE ) parcels$geoid <- structure(   rep(names(map_bg_to_parcel), vapply(map_bg_to_parcel, length, 0)),   names = names(unlist(unname(map_bg_to_parcel))) )[parcels$PIN]  estimates_parcels <- redistribute(   block_groups, parcels, map_bg_to_parcel,   target_id = \"PIN\", weight = \"Total\", default_value = 0 ) block_groups[   block_groups$GEOID %in% names(which(vapply(map_bg_to_parcel, length, 0) == 0)),   c(\"GEOID\", \"Total\"),   drop = TRUE ] #>            GEOID Total #> 16  510594405041     0 #> 72  516003003001   651 #> 84  516003005003   527 #> 131 516003002004   480 #> 132 516003005002   493 #> 212 516003004003   605 #> 237 516003002001   424 #> 305 516003001001   550 #> 317 516003002002   628 #> 318 516003001004   267 #> 383 516003001002   457 #> 494 510599801001     5 #> 532 510599802001     0 #> 537 516003003002   576 #> 538 516003003003  1008 #> 555 516003002003   395 #> 558 516003001003   776 #> 587 510599803001     0 # download parcel-level household data parcel_hh_file <- paste0(base_dir, \"/parcel_hh_va_fairfax.rds\") if (!file.exists(parcel_hh_file)) {   parcels_hh <- st_read(paste0(     \"https://opendata.arcgis.com/api/v3/datasets/\",     \"6b11da4a036049b89e656db6fe834621_0/downloads/data\",     \"?format=geojson&spatialRefId=4326\"   ))   saveRDS(parcels_hh, parcel_hh_file, compress = \"xz\") } parcels_hh <- readRDS(parcel_hh_file) rownames(parcels_hh) <- parcels_hh$PIN parcels_hh <- parcels_hh[parcels$PIN, ]  mean(abs(estimates_parcels$Total - parcels_hh$CURRE_HHLDS), na.rm = TRUE) #> [1] 0.1750593 cor(estimates_parcels$Total, parcels_hh$CURRE_HHLDS, use = \"complete.obs\") #> [1] 0.9648643 totals <- c(   \"Block Groups\" = sum(block_groups$Total),   Parcels = sum(parcels_hh$CURRE_HHLDS, na.rm = TRUE) ) data.frame(Households = c(totals, Difference = totals[[2]] - totals[[1]])) #>               Households #> Block Groups 417763.0000 #> Parcels      418711.6252 #> Difference      948.6252 map_parcel_to_zip <- redistribute(   parcels, zipcodes,   source_id = \"PIN\", target_id = \"ZIPCODE\", return_map = TRUE ) estimates_downup <- redistribute(   estimates_parcels, zipcodes, map_parcel_to_zip,   source_id = \"id\", target_id = \"ZIPCODE\", default_value = 0 )  mean(abs(estimates_downup$Total - zipcodes$HOUSEHOLDS)) #> [1] 241.4687 cor(estimates_downup$Total, zipcodes$HOUSEHOLDS) #> [1] 0.9971932 # recalculate the parcel estimates estimates_parcels_hh <- redistribute(   block_groups, parcels_hh, map_bg_to_parcel,   target_id = \"PIN\", weight = \"CURRE_HHLDS\" )  mean(abs(estimates_parcels_hh$Total - parcels_hh$CURRE_HHLDS), na.rm = TRUE) #> [1] 0.1726712 cor(estimates_parcels_hh$Total, parcels_hh$CURRE_HHLDS, use = \"complete.obs\") #> [1] 0.9675252  # reaggregate to zipcodes estimates_downup_hh <- redistribute(   estimates_parcels_hh, zipcodes, map_parcel_to_zip,   source_id = \"id\", target_id = \"ZIPCODE\", default_value = 0 )  mean(abs(estimates_downup_hh$Total - zipcodes$HOUSEHOLDS)) #> [1] 241.3931 cor(estimates_downup_hh$Total, zipcodes$HOUSEHOLDS) #> [1] 0.997208 mean(abs(parcels$Total - parcels_hh$CURRE_HHLDS), na.rm = TRUE) #> [1] 0.03345509 cor(parcels$Total, parcels_hh$CURRE_HHLDS, use = \"complete.obs\") #> [1] 0.9759002 estimates_downup_original <- redistribute(   parcels_hh, zipcodes, map_parcel_to_zip,   source_id = \"PIN\", target_id = \"ZIPCODE\", default_value = 0 )  mean(abs(estimates_downup_original$CURRE_HHLDS - zipcodes$HOUSEHOLDS)) #> [1] 0.712595 cor(estimates_downup_original$CURRE_HHLDS, zipcodes$HOUSEHOLDS) #> [1] 0.9999999"},{"path":"/articles/household_estimates.html","id":"parcels-pums","dir":"Articles","previous_headings":"","what":"Parcels + Pums","title":"Household Estimates","text":"Another thing might try using associations variables interest housing variables can derive microdata make different parcel-based estimates. First, need download prepare microdata sample: ’ll set set things calculate estimates microdata: Now can make zipcode-level estimates parcel-level estimates based PUMS associations target variables housing-related variables. ’re estimating total population, might calculate set variables measure provided zipcode-level totals:","code":"pums <- download_census_pums(   base_dir, \"va\", 2021,   one_year = FALSE, geoids = paste0(\"51\", counties) ) #> ℹ loading household sample: h.csv.xz #> ℹ loading person sample: p.csv.xz #> ℹ loading crosswalk 2010_Census_Tract_to_2010_PUMA.txt #> ℹ subsetting to 9 of 982 PUM areas pums$crosswalk$GEOID <- do.call(paste0, pums$crosswalk[, 1:3]) households <- pums$household  # prepare variables  ## survey categories households$race_cat <- paste0(\"race_\", c(   \"white\", \"black\", \"native\", rep(\"other\", 2), \"asian\", rep(\"other\", 3) ))[as.numeric(households$HHLDRRAC1P)]  households$income_cat <- as.character(cut(   households$HINCP, c(-Inf, 50, 100, 200, Inf) * 1e3, income_vars_select,   right = FALSE ))  ## unit categories households <- households[!is.na(households$BLD) & !is.na(households$TEN), ] households$building_type <- \"MULTI\" households$building_type[households$BLD == \"02\"] <- \"SFD\" households$building_type[households$BLD == \"03\"] <- \"SFA\" # define variables of interest vars_house <- c(   ID = \"SERIALNO\", weight = \"WGTP\", type = \"building_type\",   income_cat = \"income_cat\", race_cat = \"race_cat\" ) vars_units <- c(type = \"Unit_Type\")  ## get their levels return_vars <- c(\"income_cat\", \"race_cat\") vars_list <- lapply(vars_house[return_vars], function(var) unique(households[[var]])) vars <- unlist(vars_list, use.names = FALSE)  # prepare datasets split into PUMAs pumas_focal <- unique(households$PUMA) data <- lapply(structure(pumas_focal, names = pumas_focal), function(puma) {   ids <- pums$crosswalk$GEOID[pums$crosswalk$PUMA5CE == puma]   list(     households = as.data.frame(households[households$PUMA == puma, vars_house]),     source = block_groups[       substring(block_groups$GEOID, 1, 11) %in% ids, c(\"GEOID\", vars),       drop = TRUE     ],     parcels = parcels[       substring(parcels$geoid, 1, 11) %in% ids, c(\"geoid\", \"PIN\", \"Total\", vars_units),       drop = TRUE     ],     map = map_bg_to_parcel   ) })  # function to apply each method to the data apply_method <- function(data, method, rescale = FALSE) {   p <- do.call(rbind, lapply(data, function(set) {     set$source <- st_drop_geometry(set$source)     do.call(rbind, lapply(unique(set$source$GEOID), function(id) {       source <- set$source[set$source$GEOID == id, -1]       target <- set$parcels[if (\"geography\" %in% names(set)) {         set$parcels$geoid %in% set$map[[id]]       } else {         set$parcels$geoid == id       }, ]       if (nrow(target)) {         est <- if (sum(source)) method(source, target[, -1], set$households) else NULL         if (length(est)) {           if (rescale) {             totals <- colSums(est)             totals[!totals] <- 1             est <- sweep(est, 2, totals, \"/\") * rep(               as.numeric(source[, colnames(est)]),               each = nrow(est)             )           }           su <- !vars %in% colnames(est)           if (any(su)) est[, vars[su]] <- 0           cbind(target[, 1:2], est[as.character(target$PIN), vars])         } else {           est <- target[, 1:2]           est[, vars] <- 0           est         }       }     }))   }))   redistribute(     p[, -1], zipcodes, map_parcel_to_zip,     source_id = \"PIN\", target_id = \"ZIPCODE\", default_value = 0   ) }  # function to calculate estimates from weights make_estimates <- function() {   do.call(rbind, lapply(unique(target$Unit_Type), function(type) {     d <- target[target$Unit_Type == type, c(\"PIN\", \"Total\")]     nd <- nrow(d)     i <- households[       households$building_type == type, c(\"weight\", return_vars)     ]     weight_total <- sum(i$weight)     as.data.frame(do.call(cbind, lapply(return_vars, function(cat) {       props <- tapply(i$weight, i[[cat]], sum) / weight_total       props[vars_list[[cat]][!vars_list[[cat]] %in% names(props)]] <- 0       props[is.na(props)] <- 0       props <- props[vars_list[[cat]]]       matrix(         d$Total * rep(props, each = nd),         nd,         dimnames = list(d$PIN, names(props))       )     })))   })) } method_pums <- function(source, target, households) {   households$weight <- households$WGTP   environment(make_estimates) <- environment()   make_estimates() } estimates_pums <- apply_method(data, method_pums) estimates_pums$Total <- rowSums(estimates_pums[   , grep(\"^race_\", colnames(estimates_pums)),   drop = TRUE ])  mean(abs(estimates_pums$Total - zipcodes$HOUSEHOLDS)) #> [1] 552.7519 cor(estimates_pums$Total, zipcodes$HOUSEHOLDS) #> [1] 0.9678758"},{"path":"/articles/household_estimates.html","id":"parcels-raked-pums","dir":"Articles","previous_headings":"","what":"Parcels + Raked PUMS","title":"Household Estimates","text":"previous example, use ACS summary information make estimates. might incorporate summaries adjusting initial PUMS weights fit totals within block group: Totals adjusted weights : spread across categories little different: differences much smaller estimates rescaled – rescaling suppresses much difference raking makes.","code":"library(anesrake) rake_prep <- function() {   eval(expression({     for (var in names(totals)) {       households[[var]] <- as.factor(households[[var]])       l <- levels(households[[var]])       totals[[var]] <- totals[[var]][l]       su <- is.na(totals[[var]]) | totals[[var]] == 0       if (sum(su)) {         households <- households[!households[[var]] %in% l[su], ]         households[[var]] <- droplevels(households[[var]])         totals[[var]] <- totals[[var]][!su]       }       total <- sum(totals[[var]])       if (total) totals[[var]] <- totals[[var]] / total     }     totals <- Filter(length, totals)   }), parent.frame()) } method_rake <- function(source, target, households) {   totals <- lapply(     structure(names(vars_list)[1:2], names = names(vars_list)[1:2]),     function(n) unlist(source[, vars_list[[n]]])   )   rake_prep()   households$building_type <- as.factor(households$building_type)   households$income_cat <- as.factor(households$income_cat)   capture.output(households$weight <- tryCatch(     anesrake(       totals, households, households$SERIALNO, households$WGTP,       pctlim = .001     )$weightvec[as.character(households$SERIALNO)],     error = function(e) {       warning(e$message)       households$WGTP     }   ))   environment(make_estimates) <- environment()   make_estimates() } estimates_rake <- apply_method(data, method_rake) estimates_rake$Total <- rowSums(estimates_rake[   , grep(\"^race_\", colnames(estimates_rake)),   drop = TRUE ]) all.equal(estimates_rake$Total, estimates_pums$Total) #> [1] TRUE kable(data.frame(   mae = colMeans(abs(     estimates_pums[, vars, drop = TRUE] -       estimates_rake[, vars, drop = TRUE]   )),   cor = diag(cor(     estimates_pums[, vars, drop = TRUE],     estimates_rake[, vars, drop = TRUE]   )) ), digits = 4)"},{"path":"/articles/household_estimates.html","id":"comparison","dir":"Articles","previous_headings":"","what":"Comparison","title":"Household Estimates","text":"’ll start looking largest category variable (race: white) compare methods. can first check totals methods: direct methods naturally recover original totals, methods () depend parcel coverage block groups, (b) PUMS-based estimates guaranteed recover category-level totals. can also look estimates map: look correlations : Finally, might take look smallest category variable (race: native):","code":"data <- cbind(   Direct = estimates_direct$race_white,   DirectHH = estimates_direct_weighted$race_white,   DownUp = estimates_downup$race_white,   DownUpHH = estimates_downup_hh$race_white,   Pums = estimates_pums$race_white,   Rake = estimates_rake$race_white ) data.frame(   \"Race: White\" = c(Original = sum(block_groups$race_white), colSums(data)),   check.names = FALSE ) #>          Race: White #> Original    261026.0 #> Direct      261026.0 #> DirectHH    261026.0 #> DownUp      255839.0 #> DownUpHH    255839.0 #> Pums        237747.8 #> Rake        240176.3 library(leaflet) mapdata <- cbind(rmapshaper::ms_simplify(zipcodes[, 2], keep_shapes = TRUE), data) all_values <- unlist(data) pal <- colorNumeric(scico::scico(255, palette = \"lajolla\"), all_values) leaflet(   mapdata,   options = leafletOptions(attributionControl = FALSE) ) |>   addProviderTiles(\"CartoDB.Positron\") |>   addScaleBar(\"bottomleft\") |>   addControl(\"Race: White\", \"topright\") |>   addLayersControl(     position = \"topleft\",     baseGroups = c(       \"Direct\", \"Direct Weighted\", \"DownUp\", \"DownUp Households\", \"Pums\", \"Rake\"     )   ) |>   addLegend(\"bottomright\", pal, all_values, opacity = 1) |>   addPolygons(     fillColor = ~ pal(Direct), fillOpacity = 1, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Direct\", label = ~ paste(ZIPCODE, \"White:\", Direct)   ) |>   addPolygons(     fillColor = ~ pal(DirectHH), fillOpacity = 1, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Direct Weighted\", label = ~ paste(ZIPCODE, \"White:\", DirectHH)   ) |>   addPolygons(     fillColor = ~ pal(DownUp), fillOpacity = 1, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"DownUp\", label = ~ paste(ZIPCODE, \"White:\", DownUp)   ) |>   addPolygons(     fillColor = ~ pal(DownUpHH), fillOpacity = 1, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"DownUp Households\", label = ~ paste(ZIPCODE, \"White:\", DownUpHH)   ) |>   hideGroup(\"DownUp\") |>   addPolygons(     fillColor = ~ pal(Pums), fillOpacity = 1, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Pums\", label = ~ paste(ZIPCODE, \"White:\", Pums)   ) |>   hideGroup(\"Pums\") |>   addPolygons(     fillColor = ~ pal(Rake), fillOpacity = 1, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Rake\", label = ~ paste(ZIPCODE, \"White:\", Rake)   ) |>   hideGroup(\"Rake\") kable(cor(data), digits = 5) data_native <- cbind(   Direct = estimates_direct$race_native,   DirectHH = estimates_direct_weighted$race_native,   DownUp = estimates_downup$race_native,   DownUpHH = estimates_downup_hh$race_native,   Pums = estimates_pums$race_native,   Rake = estimates_rake$race_native ) data.frame(   \"Race: Native\" = c(Original = sum(block_groups$race_native), colSums(data_native)),   check.names = FALSE ) #>          Race: Native #> Original    1132.0000 #> Direct      1132.0000 #> DirectHH    1132.0000 #> DownUp      1131.0000 #> DownUpHH    1131.0000 #> Pums         761.7061 #> Rake         354.9464 kable(cor(data_native), digits = 5)"},{"path":"/articles/introduction.html","id":"basic-example","dir":"Articles","previous_headings":"","what":"Basic Example","title":"Introduction to Redistribution","text":"Say 1 set 5 regions:","code":"regions <- data.frame(id = 1:5) regions #>   id #> 1  1 #> 2  2 #> 3  3 #> 4  4 #> 5  5"},{"path":"/articles/introduction.html","id":"disaggregate","dir":"Articles","previous_headings":"Basic Example","what":"Disaggregate","title":"Introduction to Redistribution","text":"single observation entire set, disaggregate regions. additional information, best guess value region proportional split – case, one fifth observed value region: , maybe know little regions, size; use information adjust proportion allotted region:","code":"# install if needed: remotes::install_github(\"uva-bi-sdad/redistribute\") library(redistribute)  set_value <- 1 (redistribute(set_value, regions)) #>   id source #> 1  1    0.2 #> 2  2    0.2 #> 3  3    0.2 #> 4  4    0.2 #> 5  5    0.2 region_values <- redistribute(   set_value, regions,   weight = c(1, 10, 10, 20, 50) ) region_values #>   id     source #> 1  1 0.01098901 #> 2  2 0.10989011 #> 3  3 0.10989011 #> 4  4 0.21978022 #> 5  5 0.54945055"},{"path":"/articles/introduction.html","id":"aggregate","dir":"Articles","previous_headings":"Basic Example","what":"Aggregate","title":"Introduction to Redistribution","text":"observations regions, aggregate single value set. case, can re-aggregate initially disaggregated, recover original observation:","code":"(redistribute(region_values)) #>   id source #> 1  1      1"},{"path":"/articles/introduction.html","id":"applied-example","dir":"Articles","previous_headings":"","what":"Applied Example","title":"Introduction to Redistribution","text":"One use case redistribution converting demographics data geographic layers. illustration, can look U.S. Census data Fairfax, Virginia: population information provided Census Block Group level lowest, might want look population within zip codes. try aggregating directly block groups zip codes, involves calculating proportional intersections region polygons: also disaggregate parcel level, point locations, aggregate zipcodes: Now can compare estimated total population different methods provided:","code":"base_dir <- \"~/Downloads\" # remotes::install_github(\"uva-bi-sdad/catchment\") library(catchment) library(sf)  # download population and data shapes population <- download_census_population(   base_dir, \"VA\", 2020,   counties = c(\"51059\", \"51600\") )$estimates #> ℹ loading existing Virginia population data population[, -1] <- vapply(   population[, -1], as.numeric, numeric(nrow(population)) ) rownames(population) <- population$GEOID block_groups <- st_transform(   download_census_shapes(base_dir, \"VA\", \"bg\", year = 2020), \"WGS84\" ) block_groups <- block_groups[block_groups$GEOID %in% population$GEOID, ] population <- population[block_groups$GEOID, ] st_geometry(population) <- block_groups$geometry population$Overall <- population$TOTAL.POPULATION_Total # Download shapes if needed zipcode_file <- paste0(base_dir, \"/zipcode_va_fairfax.geojson\") if (!file.exists(zipcode_file)) {   download.file(paste0(     \"https://www.fairfaxcounty.gov/euclid/rest/services/IPLS/IPLSMap\",     \"/MapServer/3/query?where=1=1&outFields=*&outSR=4326&f=geojson\"   ), zipcode_file) } zipcodes <- read_sf(zipcode_file)  # redistribute population data from block groups zipcode_population <- redistribute(   population, zipcodes,   target_id = \"ZIPCODE\" ) # Download shapes if needed ## https://data-fairfaxcountygis.opendata.arcgis.com/datasets/current-population parcel_file <- paste0(base_dir, \"/parcel_va_fairfax.geojson\") if (!file.exists(parcel_file)) {   download.file(paste0(     \"https://opendata.arcgis.com/api/v3/datasets/\",     \"314bfe4019754952a715be3a33384d9d_0/downloads/data\",     \"?format=geojson&spatialRefId=4326&where=1=1\"   ), parcel_file) } parcels <- read_sf(parcel_file)  # disaggregate population data from block groups to parcels bg_parcel_population <- redistribute(   population, parcels,   weight = \"CURRE_POPUL\" )  # then aggregate from parcels to zip codes bg_parcel_zipcode_population <- redistribute(   bg_parcel_population, zipcodes,   source_id = \"id\", target_id = \"ZIPCODE\" )  # since it's provided in this case, we can also just aggregate # up from parcels directly parcel_zipcode_population <- redistribute(   parcels, zipcodes,   source_id = \"PIN\", target_id = \"ZIPCODE\" ) library(leaflet) all_values <- c(   zipcodes$POPULATION, zipcode_population$Overall,   bg_parcel_zipcode_population$Overall, parcel_zipcode_population$CURRE_POPUL ) pal <- colorNumeric(scico::scico(255, palette = \"lajolla\"), population$Overall) pal_zip <- colorNumeric(scico::scico(255, palette = \"lajolla\"), all_values) leaflet(   population,   options = leafletOptions(attributionControl = FALSE) ) |>   addProviderTiles(\"CartoDB.Positron\") |>   addScaleBar(\"bottomleft\") |>   addControl(\"Total Population\", \"topright\") |>   addLayersControl(     position = \"topleft\", overlayGroups = \"Block Groups\",     baseGroups = c(       \"Zip Codes\", \"Parcels -> Zip Codes\", \"Block Groups -> Zip Codes\",       \"Block Groups -> Parcels -> Zip Codes\"     )   ) |>   addLegend(     \"bottomright\", pal, ~Overall,     title = \"Block Groups\", opacity = 1   ) |>   addPolygons(     fillColor = ~ pal(Overall), fillOpacity = 1, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Block Groups\", label = ~ paste(       GEOID, \"Population:\", round(Overall, 3)     )   ) |>   addPolygons(     data = zipcodes,     fillColor = ~ pal_zip(POPULATION), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Zip Codes\", label = ~ paste(       ZIPCODE, \"Population:\", round(POPULATION, 3)     )   ) |>   hideGroup(\"Zip Codes\") |>   addPolygons(     data = parcel_zipcode_population,     fillColor = ~ pal_zip(CURRE_POPUL), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Parcels -> Zip Codes\", label = ~ paste(       id, \"Population:\", round(CURRE_POPUL, 3)     )   ) |>   hideGroup(\"Parcels -> Zip Codes\") |>   addPolygons(     data = zipcode_population,     fillColor = ~ pal_zip(Overall), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Block Groups -> Zip Codes\", label = ~ paste(       id, \"Population:\", round(Overall, 3)     )   ) |>   hideGroup(\"Block Groups -> Zip Codes\") |>   addPolygons(     data = bg_parcel_zipcode_population,     fillColor = ~ pal_zip(Overall), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Block Groups -> Parcels -> Zip Codes\", label = ~ paste(       id, \"Population:\", round(Overall, 3)     )   ) |>   showGroup(\"Block Groups -> Parcels -> Zip Codes\") |>   addLegend(\"bottomright\", pal_zip, all_values, opacity = 1, title = \"Zip Codes\")"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Biocomplexity Institute. Copyright holder, funder. Micah Iserman. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Iserman M (2023). redistribute: Redistribute Data. R package version 0.0.1, https://github.com/uva-bi-sdad/redistribute.","code":"@Manual{,   title = {redistribute: Redistribute Data},   author = {Micah Iserman},   year = {2023},   note = {R package version 0.0.1},   url = {https://github.com/uva-bi-sdad/redistribute}, }"},{"path":"/index.html","id":"redistribute","dir":"","previous_headings":"","what":"Redistribute Data","title":"Redistribute Data","text":"R package redistribute data. data redistributed (source) observed given frame (across rows associated IDs). data redistributed new frames (target; different rows IDs mapped source). Generally, frames represent groupings, lowest-level frame contains single observations (e.g., individual, individual single time-point, time-point single source, etc.), highest-level frame single observation entire population. example, U.S. Census Bureau releases data different geolevels, county, tract, block group, increasingly lower-level (higher-resolution). represent different frames observations might redistributed. useful observations one frame, want observations another. Frames may also roughly level (similar observation-group sizes), arranged differently (e.g., group individuals along different dimensions). case, might best redistribute data lower-level frame, back higher-level frame.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Redistribute Data","text":"Download R r-project.org, install package R console: load package:","code":"# install.packages(\"remotes\") remotes::install_github(\"uva-bi-sdad/redistribute\") library(redistribute)"},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 redistribute authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/download_census_pums.html","id":null,"dir":"Reference","previous_headings":"","what":"Download U.S. Census Microdata — download_census_pums","title":"Download U.S. Census Microdata — download_census_pums","text":"Download load U.S. census American Community Survey (ACS) Public Use Microdata Samples (PUMS): census.gov/programs-surveys/acs/microdata.html","code":""},{"path":"/reference/download_census_pums.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download U.S. Census Microdata — download_census_pums","text":"","code":"download_census_pums(dir, state, year = 2021, level = \"both\",   one_year = TRUE, calculate_error = FALSE, crosswalk = TRUE,   geoids = NULL, verbose = TRUE)"},{"path":"/reference/download_census_pums.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download U.S. Census Microdata — download_census_pums","text":"dir Directory save file(s). state Postal FIPS code state. year 4 digit year, 2005 recent year. level character indicating whether get person- household-level sample. Defaults . one_year Logical; FALSE, get 5-year estimates rather 1-year file. specified, fall back 5-year file 1-year file available. calculate_error Logical; TRUE, calculate standard errors variable using Successive Difference Replicate Formula census.gov/programs-surveys/acs/library/handbooks/pums.html. crosswalk Logical; FALSE, retrieve PUMA relationship files associating Census tracts PUM areas. treated TRUE geoids specified. geoids vector county, tract, block group GEOIDs within specified state select PUM areas ; defaults areas. specified, crosswalk treated TRUE. verbose Logical; FALSE, print status messages.","code":""},{"path":"/reference/download_census_pums.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download U.S. Census Microdata — download_census_pums","text":"list entries year state (specified), dictionary (containing data dictionary year), household /person (survey data), optionally household_error /person_error (calculate_error TRUE), crosswalk (crosswalk TRUE geoids specified).","code":""},{"path":"/reference/download_census_pums.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download U.S. Census Microdata — download_census_pums","text":"","code":"if (FALSE) { download_census_pums(\".\", \"va\") }"},{"path":"/reference/generate_population.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Population — generate_population","title":"Generate a Population — generate_population","text":"Simulate population individuals within households, complex relationships demographic location features.","code":""},{"path":"/reference/generate_population.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Population — generate_population","text":"","code":"generate_population(N = 1000, regions = NULL, capacities = NULL,   region_ids = NULL, attraction_loci = 3, random_regions = 0.1,   cost_loci = 2, size_loci = 5, similarity_metric = \"euclidean\",   n_neighbors = 50, neighbor_range = 0.5, n_races = 6,   n_building_types = 3, verbose = FALSE)"},{"path":"/reference/generate_population.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Population — generate_population","text":"N Number initial individuals generate. Final number individuals larger. regions vector region IDs, matrix coordinates, sf object geometries coordinates can derived. specified (capacities specified), regions similar housing units (mix single multi-family locations) generated. capacities vector maximum number households entry regions. region_ids vector unique IDs regions, column name regions containing IDs. attraction_loci Number locations selected centers attractiveness, influence households located. random_regions number 0 1, determines proportion people randomly relocated, case capacity households. cost_loci Number locations selected centers cost, influences initial income associated households. size_loci Number locations selected centers size, influence household sizes. similarity_metric Name metric use calculate nearness neighbors; see lma_simets. n_neighbors Number neighbors used influence new household's initial age race. neighbor_range Minimum similarity people considered neighbors, 0 1 (0 means unrestricted, 1 means region ). n_races Number different race groups sample . n_building_types Number different building types sample . verbose Logical; TRUE, show status messages.","code":""},{"path":"/reference/generate_population.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Population — generate_population","text":"list entries params (initial settings), two data.frames: households   individuals","code":""},{"path":"/reference/generate_population.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a Population — generate_population","text":"population generated two steps: First, households generated placed within regions. Placement within regions determined total distance one regions selected attraction loci. coordinates provided, first randomly generated. households placed, household incomes (first member) generated based cost loci, used generate building types (types increasingly associated income) household size (based size loci, income, building type). Renting status generated based income building type: 60% chance income mean income, 20% otherwise, multiplied .8 building type selected renting type, .3 otherwise. Second, individuals generated household. generate individual, first, neighbors searched , based n_neighbors neighbor_range. neighbors summarized: average age income, tabulated race. affect first member household: age first drawn Beta distribution (shapes 1 2 renting 1.5 otherwise, multiplied 80) added 18, adjusted toward random value centered average neighbor age (floored Gaussian standard deviation 1), race sampled (highest result Binomial draw n_races trials proportion neighbors * base rate chance success race group). Neighbors also affect income second member household first member's income neighbor mean income (40,000 given neighbors); case, second member's income drawn Gaussian distribution centered first member's income, standard deviation 10,000. second member's age based first member; floored Gaussian centered first member's age, standard deviation 15 first member's age 40, 5 otherwise, trimmed 18 90. second member's race 70% chance first member, 30% chance selected like first member's. Members second income, age randomly selected uniform distribution 0 first member's age minus 15 (rounded ), race determined either first second member (50% chance). Sex 50% chance 0 1 second member; sex 10% chance first member's, 90% chance opposite.","code":""},{"path":"/reference/generate_population.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a Population — generate_population","text":"","code":"generate_population(2) #> $params #> $params$neighbors #> [1] 50 #>  #> $params$range #> [1] 0.5 #>  #> $params$races_rates #> [1] 0.1314578 0.5000000 0.1501386 0.1792550 0.2574964 0.2206485 #>  #> $params$n_building_types #> [1] 3 #>  #>  #> $regions #>   id capacity     cost building_type      X      Y #> 1  1        1 697062.8             2 105729 102157 #> 2  2        1 618705.0             3  92783  93109 #>  #> $households #>   household region head_income size building_type renting #> 1         1      1      324560    8             2       1 #> 2         2      2      564551   10             3       0 #>  #> $individuals #>    household person neighbors age sex race income #> 1          1      1         0  57   1    3 324560 #> 2          1      2         0  85   0    3      0 #> 3          1      3         0  41   1    3      0 #> 4          1      4         0   8   0    3      0 #> 5          1      5         0  14   0    3      0 #> 6          1      6         0  37   0    3      0 #> 7          1      7         0  37   1    3      0 #> 8          1      8         0  36   1    3      0 #> 9          2      9         0  63   1    1 564551 #> 10         2     10         0  81   0    1      0 #> 11         2     11         0  24   0    1      0 #> 12         2     12         0  38   1    1      0 #> 13         2     13         0  38   1    1      0 #> 14         2     14         0   3   1    1      0 #> 15         2     15         0   1   0    1      0 #> 16         2     16         0  29   1    1      0 #> 17         2     17         0  44   0    1      0 #> 18         2     18         0  35   1    1      0 #>"},{"path":"/reference/make_parent_polygons.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Higher-Order Polygons — make_parent_polygons","title":"Make Higher-Order Polygons — make_parent_polygons","text":"Randomly combine given polygons contiguous, larger polygons.","code":""},{"path":"/reference/make_parent_polygons.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Higher-Order Polygons — make_parent_polygons","text":"","code":"make_parent_polygons(polygons, ids = NULL, n = 2, strict_n = TRUE,   n_as_min = FALSE, buffer_dist = 5e-05, min_overlap = NULL,   verbose = TRUE)"},{"path":"/reference/make_parent_polygons.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Higher-Order Polygons — make_parent_polygons","text":"polygons sf object polygons reformed. ids vector IDs length polygons, column name polygons use IDs. n Number polygons aim combine new polygon. strict_n Logical; TRUE (default), n represents number intersecting polygons select, depending availability (minimum 2). FALSE, n represents number nearest polygons (centroid) use calculating box use selecting polygons (minimum 1). n_as_min Logical; TRUE, merge parents fewer n children random neighbor. Otherwise (default), parents may fewer n children. Applies strict_n TRUE. buffer_dist Distance around initial shape set shapes, used define neighboring shapes. Applies strict_n TRUE min_overlap Minimal area overlap potential neighboring shapes buffered target shape, used define neighboring shapes. Applies strict_n TRUE verbose Logical; FALSE, print status messages.","code":""},{"path":"/reference/make_parent_polygons.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Higher-Order Polygons — make_parent_polygons","text":"list entries new (sf object containing new polygons) map (list mapping old new polygon indices).","code":""},{"path":"/reference/redistribute.html","id":null,"dir":"Reference","previous_headings":"","what":"Redistribute Data — redistribute","title":"Redistribute Data — redistribute","text":"Distribute data source frame target frame.","code":""},{"path":"/reference/redistribute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Redistribute Data — redistribute","text":"","code":"redistribute(source, target = NULL, map = list(), source_id = \"GEOID\",   target_id = source_id, weight = NULL, source_variable = NULL,   source_value = NULL, aggregate = NULL, weight_agg_method = \"auto\",   default_value = NA, outFile = NULL, overwrite = FALSE,   make_intersect_map = FALSE, overlaps = \"keep\", use_all = TRUE,   return_geometry = TRUE, return_map = FALSE, verbose = FALSE)"},{"path":"/reference/redistribute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Redistribute Data — redistribute","text":"source matrix-like object want distribute ; usually real complete dataset, often lower resolution / higher level. target matrix-like object want distribute : usually dataset want available, often higher resolution / lower level (disaggregation). Can also single number, representing number initial characters source IDs derive target IDs (useful aggregating nested groups). map list entries named source IDs (aligning IDs), containing vectors associated target IDs (indices IDs). Entries can also numeric vectors IDs names, used weigh relationship. IDs related substrings (first characters target IDs source IDs), map can automatically generated . source target contain sf geometries, map made st_intersects (st_intersects(source, target)). intersects map made, source aggregated target, map entries contain multiple target IDs, entries weighted proportion overlap source area. source_id, target_id Name column source / target, vector containing IDs. source, default first column. target, columns searched one appears relate source IDs, falling back first column. weight Name column, vector containing weights (single value apply cases), apply target disaggregating, source aggregating. Defaults unit weights (weights 1). source_variable, source_value source tall (variables spread across rows rather columns), specifies names columns source containing variable names values conversion. aggregate Logical; specified, determine whether aggregate disaggregate source target. Otherwise, TRUE source observations target observations. weight_agg_method Means aggregating weight, case target IDs contain duplicates. Options \"sum\", \"average\", \"auto\" (default; sum weight integer-like, average otherwise). default_value Value set unmapped target ID. outFile Path CSV file save results. overwrite Logical; TRUE, overwrite existing outFile. make_intersect_map Logical; TRUE, opt calculate intersect-based map rather ID-based map, seem possible. specified FALSE, never calculate intersect-based map. overlaps specified TRUE \"keep\", assign target entities mapped multiple source entities single source entity. value determines entities weight assigned, \"first\" (default), \"last\", \"random\". use_all Logical; TRUE (default), redistribute map weights sum 1. Otherwise, entities may partially weighted. return_geometry Logical; FALSE, set returned data.frame's geometry target, exists. return_map Logical; TRUE, return map, without performing redistribution. Useful want inspect automatically created map, use later call. verbose Logical; TRUE, show status messages.","code":""},{"path":"/reference/redistribute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Redistribute Data — redistribute","text":"data.frame row target_ids (identified first column, id), column variable source.","code":""},{"path":"/reference/redistribute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Redistribute Data — redistribute","text":"","code":"# minimal example source <- data.frame(a = 1, b = 2) target <- 1:5 (redistribute(source, target, verbose = TRUE)) #> ℹ source IDs: 1 #> ℹ target IDs: `target` vector #> ℹ map: all target IDs for single source #> ℹ weights: 1 #> ℹ redistributing 2 variables from 1 source to 5 targets: #> • (numb; 2) a, b #> ℹ disaggregating... #> ✔ done disaggregating [8ms] #>  #>   id   a   b #> 1  1 0.2 0.4 #> 2  2 0.2 0.4 #> 3  3 0.2 0.4 #> 4  4 0.2 0.4 #> 5  5 0.2 0.4  # multi-entity example source <- data.frame(id = c(\"a\", \"b\"), cat = c(\"aaa\", \"bbb\"), num = c(1, 2)) target <- data.frame(   id = sample(paste0(c(\"a\", \"b\"), rep(1:5, 2))),   population = sample.int(1e5, 10) ) (redistribute(source, target, verbose = TRUE)) #> ℹ source IDs: id column of `source` #> ℹ target IDs: id column of `target` #> ℹ map: first 1 character of target IDs #> ℹ weights: 1 #> ℹ redistributing 2 variables from 2 sources to 10 targets: #> • (numb; 1) num #> • (char; 1) cat #> ℹ disaggregating... #> ✔ done disaggregating [6ms] #>  #> ℹ re-converting categorical levels #>    id cat num #> 1  a1 aaa 0.2 #> 2  b1 bbb 0.4 #> 3  b5 bbb 0.4 #> 4  a3 aaa 0.2 #> 5  a4 aaa 0.2 #> 6  a5 aaa 0.2 #> 7  b3 bbb 0.4 #> 8  b4 bbb 0.4 #> 9  a2 aaa 0.2 #> 10 b2 bbb 0.4"},{"path":"/reference/redistribute_parcel_pums_adj.html","id":null,"dir":"Reference","previous_headings":"","what":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"wrapper around redistribute function, takes PUMS data additional input, uses calculates adjusted weight redistribution parcel data.","code":""},{"path":"/reference/redistribute_parcel_pums_adj.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"","code":"redistribute_parcel_pums_adj(source, target, households,   target_total = \"Total_Units\", target_indicator = \"Unit_Type\",   households_size = \"size\", households_indicator = \"BLD\",   households_id = \"SERIALNO\", person = NULL,   person_household_id = \"SERIALNO\", ...)"},{"path":"/reference/redistribute_parcel_pums_adj.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"source, target Source target redistribute. households PUMS household data. Can list household person entries, like returned download_census_pums. target_total Column name target (e.g., parcel data; equivalent vector) containing unit count target_indicator Column name logical variable target (equivalent vector) use assign adjustment values, TRUE indicates single family home. values column characters, anything matching \"MULTI\" TRUE. households_size vector household sizes, column households containing values. specified found, person used calculate household size, based hosuehold_id person_household_id. households_indicator target_indicator, households, vector (column name) BLD codes, either \"02\" \"03\" TRUE. households_id vector household IDs aligning households, column name households containing IDs. used calculate households_size necessary. person PUMS person data. used calculate households_size necessary. person_household_id vector household IDs aligning person, column name person containing IDs. used calculate households_size necessary. ... Additional arguments pass redistribute. likely need specify map, potentially source_id /target_id.","code":""},{"path":"/reference/redistribute_parcel_pums_adj.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"Result redistribute. assumed parcel-level estimates, aggregated make higher-level estimates.","code":""},{"path":"/reference/redistribute_parcel_pums_adj.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"assumed initial weights unit counts, adjusted based PUMS household data.","code":""},{"path":"/reference/redistribute_parcel_pums_adj.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"","code":"if (FALSE) { if (require(\"tidycensus\")) {   # download source, target, and household data   tracts <- tidycensus::get_acs(     year = 2021,     state = \"51\",     county = \"013\",     geography = \"tract\",     output = \"wide\",     variables = c(total = \"B01001_001\"),     geometry = TRUE   )   parcels <- sf::st_read(paste0(     \"https://arlgis.arlingtonva.us/arcgis/rest/\",     \"services/Open_Data/od_MHUD_Polygons/\",     \"FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=json\"   ))   pums <- download_census_pums(tempdir(), \"51\", geoids = tracts$GEOID)    # calculate map between source and target   map_tr_to_parcel <- redistribute(     tracts, parcels,     target_id = \"OBJECTID\", return_map = TRUE   )    # redistribute tract-level summaries to parcels,   # using a resident-estimate-adjusted weight   parcels_filled <- redistribute_parcel_pums_adj(     tracts[, -2], parcels, pums,     map = map_tr_to_parcel, target_id = \"OBJECTID\"   )    # this can also be calculated with the underlying function    ## calculate resident estimates   household_size <- tapply(     table(pums$person$SERIALNO)[pums$household$SERIALNO],     pums$household$BLD %in% c(\"02\", \"03\"),     mean,     na.rm = TRUE   )   residents <- parcels$Total_Units * household_size[     as.character(parcels$Unit_Type != \"MULTI\")   ]    ## supply as weights   parcels_filled_manual <- redistribute(     tracts[, -2], parcels, map_tr_to_parcel,     target_id = \"OBJECTID\", weight = residents   )    # either way, you could now use the result to make estimates   # at higher-level geographies by redistributing from the   # parcel-level estimates to the new target } }"}]
