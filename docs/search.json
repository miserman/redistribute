[{"path":"/articles/estimate_comparisons.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Estimate Comparisons","text":"’ll focus single county (Arlington, VA), census block groups Public Use Microdata Samples (PUMS) primary source data. Ultimately, ’ll want use methods get values unobserved geographies, comparison methods, ’ll want way test results known values. test case, can simply start higher geolevel, make estimates block group level. , ’ll need aggregate block group data tracts:","code":"# directory to save data in base_dir <- \"../estimate_comparison\" dir.create(base_dir, FALSE) # geographies library(catchment) geography_bg <- download_census_shapes(base_dir, \"va\", \"bg\", year = 2021) geography_bg <- geography_bg[grep(\"^51013\", geography_bg$GEOID), ] geography_tr <- download_census_shapes(base_dir, \"va\", \"tr\", year = 2021) geography_tr <- geography_tr[grep(\"^51013\", geography_tr$GEOID), ]  # ACS data  ## block groups block_groups <- tidycensus::get_acs(   year = 2021,   state = \"51\",   county = \"013\",   geography = \"block group\",   output = \"wide\",   variables = c(     total = \"B01001_001\",     race_white = \"B02001_002\",     race_black = \"B02001_003\",     race_native = \"B02001_004\",     race_asian = \"B02001_005\",     sex_male = \"B01001_002\",     sex_female = \"B01001_026\",     tenure_total = \"B25003_001\",     tenure_owner = \"B25003_002\",     tenure_renter = \"B25003_003\",     income_total = \"B19001_001\",     income_lt10 = \"B19001_002\",     income_10_15 = \"B19001_003\",     income_15_20 = \"B19001_004\",     income_20_25 = \"B19001_005\",     income_25_30 = \"B19001_006\",     income_30_35 = \"B19001_007\",     income_35_40 = \"B19001_008\",     income_40_45 = \"B19001_009\",     income_45_50 = \"B19001_010\",     income_50_60 = \"B19001_011\",     income_60_75 = \"B19001_012\",     income_75_100 = \"B19001_013\",     income_100_125 = \"B19001_014\",     income_125_150 = \"B19001_015\",     income_150_200 = \"B19001_016\",     income_gt200 = \"B19001_017\",     avg_hhsize = \"B25010_001\",     avg_hhsize_own = \"B25010_002\",     avg_hhsize_rent = \"B25010_003\"   ) )[, -2] #> Getting data from the 2017-2021 5-year ACS colnames(block_groups)[2] <- \"Total\" block_groups$race_other <- block_groups$Total - rowSums(   block_groups[, grep(\"^race_.*E$\", colnames(block_groups))] ) colnames(block_groups) <- sub(\"E$\", \"\", colnames(block_groups))  ### impute missing average household sizes block_groups$avg_hhsize[is.na(block_groups$avg_hhsize)] <- mean(   block_groups$avg_hhsize,   na.rm = TRUE ) su <- is.na(block_groups$avg_hhsize_own) block_groups$avg_hhsize_own[su] <- rowMeans(   block_groups[su, c(\"avg_hhsize\", \"avg_hhsize_rent\")],   na.rm = TRUE ) block_groups[is.na(block_groups)] <- 0  ### make larger income groups income_vars <- grep(\"income\", colnames(block_groups), value = TRUE) block_groups$income_lt50 <- rowSums(   block_groups[, grep(\"_(?:lt1|[1-4][05]_).*\\\\d$\", income_vars, value = TRUE)] ) block_groups$income_50_100 <- rowSums(   block_groups[, grep(\"income_[5-7].*\\\\d$\", income_vars, value = TRUE)] ) block_groups$income_100_200 <- rowSums(   block_groups[, grep(\"_1\\\\d{2}.*\\\\d$\", income_vars, value = TRUE)] ) income_vars_select <- c(   \"income_lt50\", \"income_50_100\", \"income_100_200\", \"income_gt200\" )  library(sf) rownames(geography_bg) <- geography_bg$GEOID st_geometry(block_groups) <- st_geometry(geography_bg[block_groups$GEOID, ])  # parcel data parcel_file <- paste0(base_dir, \"/parcels.rds\") if (!file.exists(parcel_file)) {   parcels <- st_read(paste0(     \"https://arlgis.arlingtonva.us/arcgis/rest/\",     \"services/Open_Data/od_MHUD_Polygons/\",     \"FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=json\"   ))   saveRDS(parcels, parcel_file, compress = \"xz\") } parcels <- readRDS(parcel_file) colnames(parcels)[colnames(parcels) == \"Total_Units\"] <- \"Total\" parcels$OBJECTID <- as.character(parcels$OBJECTID) parcels <- parcels[parcels$Year_Built <= 2021, ] tracts <- redistribute(block_groups, geography_tr)"},{"path":"/articles/estimate_comparisons.html","id":"redistribution","dir":"Articles","previous_headings":"","what":"Redistribution","title":"Estimate Comparisons","text":"baseline, first method consider proportional redistribution block group summary estimates alone.","code":""},{"path":"/articles/estimate_comparisons.html","id":"direct","dir":"Articles","previous_headings":"Redistribution","what":"Direct","title":"Estimate Comparisons","text":"tracts block groups directly: can see case, error introduce proportional information lower level; block groups varied within tract now value:","code":"# going from tracts to block groups with no information estimate_tr_to_bg <- redistribute(   tracts, block_groups,   source_id = \"id\" )  # mean absolute error between total population estimates mean(abs(estimate_tr_to_bg$Total - block_groups$Total)) #> [1] 319.8405"},{"path":"/articles/estimate_comparisons.html","id":"down-and-up","dir":"Articles","previous_headings":"Redistribution","what":"Down and Up","title":"Estimate Comparisons","text":"go tracts parcels, back block groups: Now original variation block groups:  parcel totals living units rather individuals, may able refine redistribution based heuristic estimates number people per living unit: better sense resident adjustment , can look difference made single tract: Now can look difference contained parcels block groups:  case, difference mostly comes two buildings many units get less relative weight resident values.","code":"# function to apply tract to parcel to block group redistribution redist_downup <- function(     top, bottom, middle, map_tb = NULL, map_bm = NULL,     weight = \"Total\", source_id = \"id\",     target_id = \"OBJECTID\", target_id_mid = \"GEOID\") {   to_bottom <- redistribute(     top, bottom, map_tb,     weight = weight, source_id = source_id, target_id = target_id,     default_value = 0   )   redistribute(     to_bottom, middle, map_bm,     source_id = \"id\", target_id = target_id_mid,     default_value = 0   ) }  # pre-make maps  ## from block groups to parcels map_bg_to_parcel <- redistribute(   block_groups, parcels,   target_id = \"OBJECTID\", return_map = TRUE, overlaps = \"first\" ) parcels$geoid <- structure(   rep(names(map_bg_to_parcel), vapply(map_bg_to_parcel, length, 0)),   names = names(unlist(unname(map_bg_to_parcel))) )[parcels$OBJECTID] block_groups <- block_groups[block_groups$GEOID %in% parcels$geoid, ]  ## from tracts to parcels map_tr_to_parcel <- split(unlist(unname(map_bg_to_parcel)), rep(   substring(names(map_bg_to_parcel), 1, 11), vapply(map_bg_to_parcel, length, 0) ))  # check that down from and up to the same level works all.equal(   as.data.frame(redist_downup(     block_groups, parcels, block_groups, map_bg_to_parcel, map_bg_to_parcel,     source_id = \"GEOID\"   )[, -1]),   as.data.frame(block_groups[, -1]) ) #> [1] TRUE  # redistribute estimate_redist <- redist_downup(   tracts, parcels, block_groups, map_tr_to_parcel, map_bg_to_parcel )  # mean absolute error between total population estimates mean(abs(estimate_redist$Total - block_groups$Total)) #> [1] 231.1852 # 2.5 for multi-unit parcels, 5.5 for single-unit parcels parcels$Residents <- parcels$Total * (2.5 + (parcels$Total == 1) * 3)  # now see if this improved our redistribution estimate_redist_adj <- redist_downup(   tracts, parcels, block_groups, map_tr_to_parcel, map_bg_to_parcel,   weight = \"Residents\" )  # mean absolute error between total population estimates mean(abs(estimate_redist_adj$Total - block_groups$Total)) #> [1] 194.0749 # identify the most different tract between variants tract <- substring(block_groups$GEOID[   which.max(abs(estimate_redist_adj$Total - estimate_redist$Total)) ], 1, 11) su <- substring(estimate_redist$id, 1, 11) == tract tsu <- tracts$id == tract psu <- substring(parcels$geoid, 1, 11) == tract  # recalculate parcel-level values within that tract differences <- list(   parcels = redistribute(     tracts$Total[tsu], parcels[psu, ],     target_id = \"OBJECTID\", weight = \"Residents\", return_geometry = FALSE   )[, 2] - redistribute(     tracts$Total[tsu], parcels[psu, ],     target_id = \"OBJECTID\", weight = \"Total\", return_geometry = FALSE   )[, 2],   block_group = estimate_redist_adj$Total[su] - estimate_redist$Total[su] )"},{"path":"/articles/estimate_comparisons.html","id":"pums-raking","dir":"Articles","previous_headings":"","what":"PUMS + Raking","title":"Estimate Comparisons","text":"far, ’ve used additional information (total units) parcel level improve distribution values across block groups, relative uniform distribution. information parcel level, unit type number renter versus owner designations, make use information, need association source values features. census microdata sample can come : Public Use Microdata Sample (PUMS) consists individual-level observations include demographic features housing features. PUM sample located PUM areas, made tracts, can start baseline. use associations demographic housing features within PUMA get probabilities parcel level, redistribute source values like . first step end prepare data easy associate individual PUMS variables per-level summaries tract level: Now can apply method set data. new methods, can also update heuristic adjustment proportional redistribution method PUMS information:","code":"pums <- download_census_pums(base_dir, \"va\", 2021, one_year = FALSE) #> ℹ loading household sample: h.csv.xz #> ℹ loading person sample: p.csv.xz #> ℹ loading crosswalk 2020_Census_Tract_to_2020_PUMA.txt  # prepare IDs pums$crosswalk$GEOID <- do.call(paste0, pums$crosswalk[, 1:3]) pums$person$ID <- do.call(paste0, pums$person[, c(\"SERIALNO\", \"SPORDER\")])  # prepare variables  ## survey categories pums$person$sex_cat <- c(\"sex_male\", \"sex_female\")[as.numeric(pums$person$SEX)]  pums$person$race_cat <- paste0(\"race_\", c(   \"white\", \"black\", \"native\", rep(\"other\", 2), \"asian\", rep(\"other\", 3) ))[as.numeric(pums$person$RAC1P)]  pums$household$income_cat <- as.character(cut(   pums$household$HINCP, c(-Inf, 50, 100, 200, Inf) * 1e3, income_vars_select,   right = FALSE ))  ## unit categories pums$household <- pums$household[   !is.na(pums$household$BLD) & !is.na(pums$household$TEN), ] pums$household$building_type <- \"MULTI\" pums$household$building_type[pums$household$BLD == \"02\"] <- \"SFD\" pums$household$building_type[pums$household$BLD == \"03\"] <- \"SFA\"  pums$household$status <- \"RENTER\" pums$household$status[pums$household$TEN %in% 1:2] <- \"OWNER\" # define variables of interest vars_house <- c(   ID = \"SERIALNO\", weight = \"WGTP\", type = \"building_type\",   status = \"status\", income = \"income_cat\" ) vars_person <- c(   ID = \"ID\", weight = \"PWGTP\", sex_cat = \"sex_cat\", race_cat = \"race_cat\" ) vars_units <- c(   type = \"Unit_Type\", renters = \"Renter_Occupied\", owners = \"Owner_Occupied\" )  ## get their levels vars_list <- c(   lapply(vars_person[-(1:2)], function(var) unique(pums$person[[var]])),   income_cat = list(unique(pums$household$income_cat)) ) return_vars <- names(vars_list) pvars <- unlist(vars_list, use.names = FALSE) vars <- c(pvars, \"avg_hhsize_own\", \"avg_hhsize_rent\")  # prepare datasets split into PUMAs pumas_focal <- unique(pums$crosswalk$PUMA5CE[pums$crosswalk$GEOID %in% tracts$id]) data <- lapply(structure(pumas_focal, names = pumas_focal), function(puma) {   households <- as.data.frame(pums$household[pums$household$PUMA == puma, vars_house])   rownames(households) <- households$SERIALNO   ids <- pums$crosswalk$GEOID[     pums$crosswalk$PUMA5CE == puma & pums$crosswalk$GEOID %in% tracts$id   ]   person <- pums$person[pums$person$SERIALNO %in% households$SERIALNO, ]   list(     individuals = cbind(households[person$SERIALNO, ], person),     source = tracts[tracts$id %in% ids, c(\"id\", vars), drop = TRUE],     parcels = parcels[       substring(parcels$geoid, 1, 11) %in% ids, c(\"geoid\", \"OBJECTID\", vars_units),       drop = TRUE     ]   ) })  # function to apply each method to the data apply_method <- function(data, method, parcel_only = FALSE, rescale = TRUE) {   p <- do.call(rbind, lapply(data, function(set) {     set$source <- st_drop_geometry(set$source)     do.call(rbind, lapply(unique(set$source$id), function(id) {       source <- set$source[set$source$id == id, -1]       target <- set$parcels[if (\"map\" %in% names(set)) {         if (\"geography\" %in% names(set)) {           set$parcels$geoid %in% set$map[[id]]         } else {           set$parcels$geoid == id         }       } else {         substring(set$parcels$geoid, 1, 11) == id       }, ]       if (nrow(target)) {         est <- if (sum(source)) method(source, target[, -1], set$individuals) else NULL         if (length(est)) {           if (rescale) {             totals <- colSums(est)             totals[!totals] <- 1             est <- sweep(est, 2, totals, \"/\") * rep(               as.numeric(source[, colnames(est)]),               each = nrow(est)             )           }           su <- !pvars %in% colnames(est)           if (any(su)) est[, pvars[su]] <- 0           cbind(target[, 1:2], est[as.character(target$OBJECTID), pvars])         } else {           est <- target[, 1:2]           est[, pvars] <- 0           est         }       }     }))   }))   if (parcel_only) {     p   } else {     list(parcels = p, block_groups = redistribute(       p[, -1], block_groups, map_bg_to_parcel,       source_id = \"OBJECTID\", target_id = \"GEOID\", default_value = 0     ))   } }  # function to calculate estimates from weights make_estimates <- function() {   do.call(rbind, lapply(unique(target$Unit_Type), function(type) {     d <- target[       target$Unit_Type == type, c(\"OBJECTID\", \"Renter_Occupied\", \"Owner_Occupied\")     ]     d$Renter_Occupied <- d$Renter_Occupied * source$avg_hhsize_rent     d$Owner_Occupied <- d$Owner_Occupied * source$avg_hhsize_own     nd <- nrow(d)     colnames(d)[-1] <- c(\"RENTER\", \"OWNER\")     i <- individuals[       individuals$building_type == type, c(\"weight\", \"status\", return_vars)     ]     as.data.frame(Reduce(\"+\", lapply(       (if (is.factor(i$status)) levels else unique)(i$status),       function(s) {         ii <- i[i$status == s, ]         weight_total <- sum(ii$weight)         do.call(cbind, lapply(return_vars, function(cat) {           props <- tapply(ii$weight, ii[[cat]], sum) / weight_total           props[vars_list[[cat]][!vars_list[[cat]] %in% names(props)]] <- 0           props[is.na(props)] <- 0           props <- props[vars_list[[cat]]]           matrix(             d[[s]] * rep(props, each = nd),             nd,             dimnames = list(d$OBJECTID, names(props))           )         }))       }     )))   })) } # make single-family-home indicators pums$household$single_family_home <- pums$household$building_type != \"MULTI\" parcels$single_family_home <- parcels$Unit_Type != \"MULTI\"  # update resident estimates with average household size for # single family versus other home types within the county pums$household$size <- table(pums$person$SERIALNO)[pums$household$SERIALNO] households <- pums$household[pums$household$PUMA %in% pumas_focal, ] parcels$Residents <- parcels$Total * tapply(   households$size, households$single_family_home, mean )[as.character(parcels$single_family_home)]  # recalculate the adjusted redistribution results estimate_redist_adj <- redist_downup(   tracts, parcels, block_groups, map_tr_to_parcel, map_bg_to_parcel,   weight = \"Residents\" ) mean(abs(estimate_redist_adj$Total - block_groups$Total)) #> [1] 201.2044"},{"path":"/articles/estimate_comparisons.html","id":"initial-weights","dir":"Articles","previous_headings":"PUMS + Raking","what":"Initial Weights","title":"Estimate Comparisons","text":"baseline, can calculate set proportions PUMA, apply parcels contained tract: method make estimates recover initial values, rescale comparable. illustration:","code":"# function to apply the method: # - source: the tract-level totals # - target: the parcels # - individuals: the individual PUM responses est_baseline <- function(source, target, individuals) {   individuals$weight <- individuals$PWGTP   environment(make_estimates) <- environment()   make_estimates() }  # apply across PUMAs and tracts within them # and aggregate to block groups estimate_baseline <- apply_method(data, est_baseline) # adjust tract counts so they match # unit totals * average household size across parcels tr_adj <- st_drop_geometry(tracts) pid <- substring(parcels$geoid, 1, 11) parcels$renter_est <- parcels$Renter_Occupied * structure(   tracts$avg_hhsize_rent,   names = tracts$id )[pid] parcels$owner_est <- parcels$Owner_Occupied * structure(   tracts$avg_hhsize_own,   names = tracts$id )[pid] parcels$resident_est <- parcels$renter_est + parcels$owner_est tr_adj$Total <- tapply(parcels$resident_est, pid, sum)[tr_adj$id]  for (g in vars_list) {   tr_adj[, g] <- tr_adj[, g] / rowSums(tr_adj[, g]) * tr_adj$Total } tr_adj[is.na(tr_adj)] <- 0  # make baseline parcel-level estimates without rescaling data_adj <- data data_adj[[1]]$source <- tr_adj[tr_adj$id %in% data[[1]]$source$id, c(\"id\", vars)] data_adj[[2]]$source <- tr_adj[tr_adj$id %in% data[[2]]$source$id, c(\"id\", vars)] parcel_adj <- apply_method(   data_adj, est_baseline,   rescale = FALSE, parcel_only = TRUE )  # aggregate back up to tracts and measure per-category error tr_est <- redistribute(   parcel_adj, tr_adj, map_tr_to_parcel,   source_id = \"OBJECTID\", target_id = \"id\", default_value = 0 ) data.frame(MAE = round(vapply(   pvars, function(var) mean(abs(tr_est[[var]] - tr_adj[[var]])), 0 ), 5)) #>                      MAE #> sex_female     324.83377 #> sex_male       324.83377 #> race_black     493.47636 #> race_white     955.89524 #> race_other     671.14696 #> race_asian     434.54822 #> race_native     49.05462 #> income_lt50    761.46177 #> income_gt200   706.96859 #> income_100_200 780.96264 #> income_50_100  640.36018  ## this method does recover the parcel-level totals and overall totals g <- c(\"sex_female\", \"sex_male\") all(abs(rowSums(parcel_adj[, g]) - structure(   parcels$resident_est,   names = parcels$OBJECTID )[parcel_adj$OBJECTID]) < 1e-12) #> [1] TRUE c(   original = sum(     rbind(data_adj[[1]]$source[, g], data_adj[[2]]$source[, g])   ),   parcel = sum(parcel_adj[, g]),   tract = sum(tr_est[, g]) ) #> original   parcel    tract  #> 866301.2 866301.2 866301.2  # rescaling makes the totals line up parcel_adj_rescaled <- apply_method(   data_adj, est_baseline,   parcel_only = TRUE ) tr_est_rescaled <- redistribute(   parcel_adj_rescaled, tr_adj, map_tr_to_parcel,   source_id = \"OBJECTID\", target_id = \"id\", default_value = 0 ) data.frame(MAE = round(vapply(   pvars, function(var) mean(abs(tr_est_rescaled[[var]] - tr_adj[[var]])), 0 ), 5)) #>                MAE #> sex_female       0 #> sex_male         0 #> race_black       0 #> race_white       0 #> race_other       0 #> race_asian       0 #> race_native      0 #> income_lt50      0 #> income_gt200     0 #> income_100_200   0 #> income_50_100    0  # just like the redistribution methods tr_redist <- redist_downup(   tr_adj, parcels, tr_adj, map_tr_to_parcel, map_tr_to_parcel ) data.frame(MAE = round(vapply(   pvars, function(var) mean(abs(tr_redist[[var]] - tr_adj[[var]])), 0 ), 5)) #>                MAE #> sex_female       0 #> sex_male         0 #> race_black       0 #> race_white       0 #> race_other       0 #> race_asian       0 #> race_native      0 #> income_lt50      0 #> income_gt200     0 #> income_100_200   0 #> income_50_100    0"},{"path":"/articles/estimate_comparisons.html","id":"raking","dir":"Articles","previous_headings":"PUMS + Raking","what":"Raking","title":"Estimate Comparisons","text":"baseline, ’re using PUMS weights calculate proportions, weights meant bring totals line PUMA-level estimates; Since lower-level information form tract-level totals, can adjust weights match smaller-area summary:","code":"library(anesrake) rake_prep <- function() {   eval(expression({     for (var in names(totals)) {       individuals[[var]] <- as.factor(individuals[[var]])       l <- levels(individuals[[var]])       totals[[var]] <- totals[[var]][l]       su <- is.na(totals[[var]]) | totals[[var]] == 0       if (sum(su)) {         individuals <- individuals[!individuals[[var]] %in% l[su], ]         individuals[[var]] <- droplevels(individuals[[var]])         totals[[var]] <- totals[[var]][!su]       }       total <- sum(totals[[var]])       if (total) totals[[var]] <- totals[[var]] / total     }     totals <- Filter(length, totals)   }), parent.frame()) } est_rake <- function(source, target, individuals) {   totals <- lapply(     structure(names(vars_list)[1:2], names = names(vars_list)[1:2]),     function(n) unlist(source[, vars_list[[n]]])   )   rake_prep()   individuals$status <- as.factor(individuals$status)   individuals$building_type <- as.factor(individuals$building_type)   individuals$income_cat <- as.factor(individuals$income_cat)   capture.output(individuals$weight <- tryCatch(     anesrake(       totals, individuals, individuals$ID, individuals$PWGTP,       pctlim = .001     )$weightvec[as.character(individuals$ID)],     error = function(e) {       warning(e$message)       individuals$PWGTP     }   ))   environment(make_estimates) <- environment()   make_estimates() } estimate_rake <- apply_method(data, est_rake)"},{"path":"/articles/estimate_comparisons.html","id":"two-step-raking","dir":"Articles","previous_headings":"PUMS + Raking","what":"Two Step Raking","title":"Estimate Comparisons","text":"initial raking approach, considered person-level variables, also household-level information, might try account well first adjusting household weights, using adjusted weights initialize adjusted person-level weights:","code":"est_twostep <- function(source, target, individuals) {   all <- individuals   individuals <- individuals[!duplicated(individuals$SERIALNO), ]   # step 1: initial weights at household level   totals <- list(     income_cat = unlist(source[, grep(\"^income_\", colnames(source))]),     status = structure(       colSums(target[, vars_units[-1]]),       names = c(\"RENTER\", \"OWNER\")     ),     building_type = tapply(rowSums(target[, vars_units[-1]]), target$Unit_Type, sum)   )   rake_prep()   capture.output(initial <- tryCatch(     anesrake(       totals, individuals, individuals$SERIALNO, individuals$WGTP,       pctlim = .001     )$weightvec,     error = function(e) {       warning(e$message)       structure(individuals$WGTP, names = individuals$SERIALNO)     }   ))   all$weight <- initial[as.character(all$SERIALNO)]   all <- all[!is.na(all$weight), ]   individuals <- all   # step 2: adjust at person level   totals <- lapply(     structure(names(vars_list)[1:2], names = names(vars_list)[1:2]),     function(n) unlist(source[, vars_list[[n]]])   )   rake_prep()   initial <- individuals$weight   capture.output(individuals$weight <- tryCatch(     anesrake(       totals, individuals, individuals$ID,       individuals$PWGTP * initial / individuals$WGTP,       pctlim = .001     )$weightvec[as.character(individuals$ID)],     error = function(e) {       warning(e$message)       individuals$PWGTP * initial / individuals$WGTP     }   ))   environment(make_estimates) <- environment()   make_estimates() } estimate_twostep <- apply_method(data, est_twostep)"},{"path":"/articles/estimate_comparisons.html","id":"n-step-raking","dir":"Articles","previous_headings":"PUMS + Raking","what":"N-Step Raking","title":"Estimate Comparisons","text":"can take two-step method repeating process try better fit household individual data:","code":"est_nstep <- function(source, target, individuals) {   individuals$weight <- individuals$PWGTP   all <- individuals   iter <- function() {     individuals <- individuals[!duplicated(individuals$SERIALNO), ]     # step 1: initial weights at household level     totals <- list(       income_cat = unlist(source[, grep(\"^income_\", colnames(source))]),       status = structure(         colSums(target[, vars_units[-1]]),         names = c(\"RENTER\", \"OWNER\")       ),       building_type = tapply(rowSums(target[, vars_units[-1]]), target$Unit_Type, sum)     )     rake_prep()     household_totals <- totals     capture.output(initial <- tryCatch(       anesrake(         totals, individuals, individuals$SERIALNO,         individuals$weight / individuals$PWGTP * individuals$WGTP,         pctlim = .001       )$weightvec,       error = function(e) {         warning(e$message)         structure(           individuals$weight / individuals$PWGTP * individuals$WGTP,           names = individuals$SERIALNO         )       }     ))     all$weight <- initial[as.character(all$SERIALNO)]     all <- all[!is.na(all$weight), ]     individuals <- all     # step 2: adjust at person level     totals <- lapply(       structure(names(vars_list)[1:2], names = names(vars_list)[1:2]),       function(n) unlist(source[, vars_list[[n]]])     )     rake_prep()     initial <- individuals$weight     capture.output(individuals$weight <- tryCatch(       anesrake(         totals, individuals, individuals$ID,         individuals$PWGTP * initial / individuals$WGTP,         pctlim = .001       )$weightvec[as.character(individuals$ID)],       error = function(e) {         warning(e$message)         individuals$PWGTP * initial / individuals$WGTP       }     ))     hh <- individuals[!duplicated(individuals$SERIALNO), ]     weight <- hh$PWGTP / hh$weight * hh$WGTP     list(       error = mean(vapply(names(household_totals), function(v) {         mean((household_totals[[v]] - tapply(weight, hh[[v]], sum) / sum(weight))^2)       }, 0)),       data = individuals     )   }   previous_error <- 0   for (i in 1:20) {     step <- iter()     individuals <- step$data     if (step$error < .001 || abs(previous_error - step$error) < .0005) break     previous_error <- step$error   }   environment(make_estimates) <- environment()   make_estimates() } estimate_nstep <- apply_method(data, est_nstep)"},{"path":"/articles/estimate_comparisons.html","id":"generalized-raking","dir":"Articles","previous_headings":"PUMS + Raking","what":"Generalized Raking","title":"Estimate Comparisons","text":"two-step approach still mainly focuses better aligning person-level weights – household totals match first step, considered second step. Alternative , might try match household- person-level weights time generalized raking approach:","code":"library(mlfit) est_grake <- function(source, target, individuals) {   totals <- list(     person = sum(individuals$PWGTP),     household = sum(individuals$WGTP[!duplicated(individuals$SERIALNO)])   )   person <- lapply(names(vars_list)[1:2], function(n) {     l <- vars_list[[n]]     count <- as.numeric(source[, l, drop = TRUE])     count[!count] <- 1     count <- count / sum(count) * totals$person     s <- data.frame(level = l, count = count)     colnames(s)[1] <- n     s   })   household <- unique(individuals$building_type)   household <- structure(numeric(length(household)), names = household)   observed_types <- tapply(     rowSums(target[, vars_units[-1]]), target$Unit_Type, sum   )   household[names(observed_types)] <- observed_types   household <- list(data.frame(     building_type = names(household), count = household   ))   household[[1]]$count[is.na(household[[1]]$count)] <- 1   household[[2]] <- data.frame(     status = c(\"RENTER\", \"OWNER\"),     count = colSums(target[, c(\"Renter_Occupied\", \"Owner_Occupied\")])   )   household[[3]] <- unlist(source[, grep(\"^income_\", colnames(source))])   household[[3]] <- data.frame(     income_cat = names(household[[3]]), count = household[[3]]   )   names(household) <- c(\"building_type\", \"status\", \"income_cat\")   household <- lapply(names(household), function(var) {     l <- household[[var]]     l <- l[l[[1]] %in% unique(individuals[, var]), ]     l[[2]][!l[[2]]] <- 1     l[[2]] <- l[[2]] / sum(l[[2]]) * totals$household     l   })   m <- ml_problem(     individuals,     field_names = special_field_names(\"SERIALNO\", \"ID\", count = \"count\"),     group_controls = household,     individual_controls = person,     prior_weights = individuals$WGTP   )   fm <- tryCatch(     suppressWarnings(ml_fit_dss(m, ginv = solve)),     error = function(e) NULL   )   if (!isTRUE(fm$success)) {     fm <- tryCatch(       suppressWarnings(ml_fit_ipu(m)),       error = function(e) NULL     )   }   individuals$weight <- if (is.null(fm$weight)) {     individuals$PWGTP   } else {     fm$weight   }   environment(make_estimates) <- environment()   make_estimates() } estimate_grake <- apply_method(data, est_grake)"},{"path":[]},{"path":"/articles/estimate_comparisons.html","id":"tract-aggregates","dir":"Articles","previous_headings":"Comparisons","what":"Tract Aggregates","title":"Estimate Comparisons","text":"Now can compare considered methods within variables used inform raking approaches: Full results presented data site: redistribution_arlington","code":"error <- do.call(rbind, lapply(structure(pvars, names = pvars), function(var) {   d <- cbind(     Prop = estimate_redist[[var]],     \"Prop Adj\" = estimate_redist_adj[[var]],     Baseline = estimate_baseline$block_groups[[var]],     Rake = estimate_rake$block_groups[[var]],     \"Rake 2-Step\" = estimate_twostep$block_groups[[var]],     \"Rake N-Step\" = estimate_nstep$block_groups[[var]],     \"Rake General\" = estimate_grake$block_groups[[var]]   )   colMeans(abs(d - block_groups[[var]])) })) error <- rbind(error, Average = colMeans(error)) kable(error, digits = 3)"},{"path":"/articles/estimate_comparisons.html","id":"alternate-geolevels","dir":"Articles","previous_headings":"Comparisons","what":"Alternate Geolevels","title":"Estimate Comparisons","text":"far, ’ve used geolevel-related test-case assess methods, may effect . alternative, might abandon canonical tracts favor randomly agglutinated block groups. can also take look virtual tracts created run: Since virtual tracts randomly created, might also look results across runs sense variation:","code":"# randomly combine adjacent block groups into pairs of 2 (were possible) # apply within each PUMA apply_altgeo <- function(run = NULL, return_full = FALSE) {   vdata <- lapply(     structure(pumas_focal, names = pumas_focal),     function(puma) {       d <- data[[puma]]       su <- substring(block_groups$GEOID, 1, 11) %in% d$source$id       vtr <- make_parent_polygons(         block_groups[su, ], \"GEOID\",         n_as_min = TRUE, verbose = FALSE       )       names(vtr$new) <- names(vtr$map) <- paste0(         puma, \"_\", seq_along(vtr$new)       )       d$geography <- vtr$new       d$map <- vtr$map       d$source <- redistribute(         block_groups[su, c(\"GEOID\", vars)], vtr$new, vtr$map,         default_value = 0       )       d     }   )   vsource <- do.call(rbind, lapply(vdata, \"[[\", \"source\"))   vmap <- redistribute(     vsource, parcels,     source_id = \"id\", target_id = \"OBJECTID\", return_map = TRUE   )   vestimates <- list(     Prop = list(block_groups = redist_downup(       vsource, parcels, block_groups, vmap, map_bg_to_parcel     )),     \"Prop Adj\" = list(block_groups = redist_downup(       vsource, parcels, block_groups, vmap, map_bg_to_parcel,       weight = \"Residents\"     )),     Baseline = apply_method(vdata, est_baseline),     Rake = apply_method(vdata, est_rake),     \"Rake 2-Step\" = apply_method(vdata, est_twostep),     \"Rake N-Step\" = apply_method(vdata, est_nstep),     \"Rake General\" = apply_method(vdata, est_grake)   )   verror <- do.call(rbind, lapply(     structure(pvars, names = pvars),     function(var) {       colMeans(abs(as.data.frame(         lapply(vestimates, function(d) d$block_groups[[var]]),         check.names = FALSE       ) - block_groups[[var]]))     }   ))   verror <- rbind(verror, Average = colMeans(verror))   if (return_full) {     list(       data = data, source = vsource, map = vmap,       estimates = vestimates, error = verror     )   } else {     verror   } }  estimates_altgeo <- apply_altgeo(return_full = TRUE) kable(estimates_altgeo$error, digits = 3) # since this can be quite long-running, # we'll save and reload results results_altgeo_file <- paste0(base_dir, \"/results_altgeo.rds\") if (file.exists(results_altgeo_file)) {   results_altgeo <- readRDS(results_altgeo_file) } else {   results_altgeo <- lapply(1:50, apply_altgeo)   saveRDS(results_altgeo, results_altgeo_file) }  kable(Reduce(\"+\", results_altgeo) / length(results_altgeo), digits = 3)"},{"path":"/articles/introduction.html","id":"basic-example","dir":"Articles","previous_headings":"","what":"Basic Example","title":"Introduction to Redistribution","text":"Say 1 set 5 regions:","code":"regions <- data.frame(id = 1:5) regions #>   id #> 1  1 #> 2  2 #> 3  3 #> 4  4 #> 5  5"},{"path":"/articles/introduction.html","id":"disaggregate","dir":"Articles","previous_headings":"Basic Example","what":"Disaggregate","title":"Introduction to Redistribution","text":"single observation entire set, disaggregate regions. additional information, best guess value region proportional split – case, one fifth observed value region: , maybe know little regions, size; use information adjust proportion allotted region:","code":"# install if needed: remotes::install_github(\"uva-bi-sdad/redistribute\") library(redistribute)  set_value <- 1 (redistribute(set_value, regions)) #>   id  V1 #> 1  1 0.2 #> 2  2 0.2 #> 3  3 0.2 #> 4  4 0.2 #> 5  5 0.2 region_values <- redistribute(   set_value, regions,   weight = c(1, 10, 10, 20, 50) ) region_values #>   id         V1 #> 1  1 0.01098901 #> 2  2 0.10989011 #> 3  3 0.10989011 #> 4  4 0.21978022 #> 5  5 0.54945055"},{"path":"/articles/introduction.html","id":"aggregate","dir":"Articles","previous_headings":"Basic Example","what":"Aggregate","title":"Introduction to Redistribution","text":"observations regions, aggregate single value set. case, can re-aggregate initially disaggregated, recover original observation:","code":"(redistribute(region_values)) #>   id V1 #> 1  1  1"},{"path":"/articles/introduction.html","id":"applied-example","dir":"Articles","previous_headings":"","what":"Applied Example","title":"Introduction to Redistribution","text":"One use case redistribution converting demographics data geographic layers. illustration, can look U.S. Census data Fairfax, Virginia: population information provided Census Block Group level lowest, might want look population within zip codes. try aggregating directly block groups zip codes, involves calculating proportional intersections region polygons: also disaggregate parcel level, point locations, aggregate zipcodes: Now can compare estimated total population different methods provided:","code":"base_dir <- \"~/Downloads\" # remotes::install_github(\"uva-bi-sdad/catchment\") library(catchment) library(sf)  # download population and data shapes population <- download_census_population(base_dir, \"VA\", 2020)$estimates #> ℹ loading existing Virginia population data population <- population[grepl(\"^51(?:059|600)\", population$GEOID), ] population[, -1] <- vapply(   population[, -1], as.numeric, numeric(nrow(population)) ) rownames(population) <- population$GEOID block_groups <- st_transform(   download_census_shapes(base_dir, \"VA\", \"bg\", year = 2020), \"WGS84\" ) block_groups <- block_groups[block_groups$GEOID %in% population$GEOID, ] population <- population[block_groups$GEOID, ] st_geometry(population) <- block_groups$geometry population$Overall <- population$TOTAL.POPULATION_Total  # prepare a map library(leaflet) pal <- colorNumeric(scico::scico(255, palette = \"lajolla\"), population$Overall) map <- leaflet(   population,   options = leafletOptions(attributionControl = FALSE) ) |>   addProviderTiles(\"CartoDB.Positron\") |>   addScaleBar(\"bottomleft\") |>   addControl(\"Total Population\", \"topright\") |>   addLayersControl(     position = \"topleft\", overlayGroups = \"Block Groups\",     baseGroups = c(       \"Zip Codes\", \"Parcels -> Zip Codes\", \"Block Groups -> Zip Codes\",       \"Block Groups -> Parcels -> Zip Codes\"     )   ) |>   addLegend(     \"bottomright\", pal, ~Overall,     title = \"Block Groups\", opacity = 1   ) |>   addPolygons(     fillColor = ~ pal(Overall), fillOpacity = 1, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Block Groups\", label = ~ paste(       GEOID, \"Population:\", round(Overall, 3)     )   ) # Download shapes if needed zipcode_file <- paste0(base_dir, \"/zipcode_va_fairfax.geojson\") if (!file.exists(zipcode_file)) {   download.file(paste0(     \"https://www.fairfaxcounty.gov/euclid/rest/services/IPLS/IPLSMap\",     \"/MapServer/3/query?where=1=1&outFields=*&outSR=4326&f=geojson\"   ), zipcode_file) } zipcodes <- read_sf(zipcode_file)  # redistribute population data from block groups zipcode_population <- redistribute(   population, zipcodes,   target_id = \"ZIPCODE\" ) # Download shapes if needed ## https://data-fairfaxcountygis.opendata.arcgis.com/datasets/current-population parcel_file <- paste0(base_dir, \"/parcel_va_fairfax.geojson\") if (!file.exists(parcel_file)) {   download.file(paste0(     \"https://opendata.arcgis.com/api/v3/datasets/\",     \"314bfe4019754952a715be3a33384d9d_0/downloads/data\",     \"?format=geojson&spatialRefId=4326&where=1=1\"   ), parcel_file) } parcels <- read_sf(parcel_file)  # disaggregate population data from block groups to parcels bg_parcel_population <- redistribute(   population, parcels,   weight = \"CURRE_POPUL\" )  # then aggregate from parcels to zip codes bg_parcel_zipcode_population <- redistribute(   bg_parcel_population, zipcodes,   source_id = \"id\", target_id = \"ZIPCODE\" )  # since it's provided in this case, we can also just aggregate # up from parcels directly parcel_zipcode_population <- redistribute(   parcels, zipcodes,   source_id = \"PIN\", target_id = \"ZIPCODE\" ) all_values <- c(   zipcodes$POPULATION, zipcode_population$Overall,   bg_parcel_zipcode_population$Overall, parcel_zipcode_population$CURRE_POPUL ) pal_zip <- colorNumeric(scico::scico(255, palette = \"lajolla\"), all_values) map |>   addPolygons(     data = zipcodes,     fillColor = ~ pal_zip(POPULATION), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Zip Codes\", label = ~ paste(       ZIPCODE, \"Population:\", round(POPULATION, 3)     )   ) |>   hideGroup(\"Zip Codes\") |>   addPolygons(     data = parcel_zipcode_population,     fillColor = ~ pal_zip(CURRE_POPUL), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Parcels -> Zip Codes\", label = ~ paste(       id, \"Population:\", round(CURRE_POPUL, 3)     )   ) |>   hideGroup(\"Parcels -> Zip Codes\") |>   addPolygons(     data = zipcode_population,     fillColor = ~ pal_zip(Overall), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Block Groups -> Zip Codes\", label = ~ paste(       id, \"Population:\", round(Overall, 3)     )   ) |>   hideGroup(\"Block Groups -> Zip Codes\") |>   addPolygons(     data = bg_parcel_zipcode_population,     fillColor = ~ pal_zip(Overall), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Block Groups -> Parcels -> Zip Codes\", label = ~ paste(       id, \"Population:\", round(Overall, 3)     )   ) |>   showGroup(\"Block Groups -> Parcels -> Zip Codes\") |>   addLegend(\"bottomright\", pal_zip, all_values, opacity = 1, title = \"Zip Codes\")"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Biocomplexity Institute. Copyright holder, funder. Micah Iserman. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Iserman M (2023). redistribute: Redistribute Data. R package version 0.0.1, https://github.com/uva-bi-sdad/redistribute.","code":"@Manual{,   title = {redistribute: Redistribute Data},   author = {Micah Iserman},   year = {2023},   note = {R package version 0.0.1},   url = {https://github.com/uva-bi-sdad/redistribute}, }"},{"path":"/index.html","id":"redistribute","dir":"","previous_headings":"","what":"Redistribute Data","title":"Redistribute Data","text":"R package redistribute data. data redistributed (source) observed given frame (across rows associated IDs). data redistributed new frames (target; different rows IDs mapped source). Generally, frames represent groupings, lowest-level frame contains single observations (e.g., individual, individual single time-point, time-point single source, etc.), highest-level frame single observation entire population. example, U.S. Census Bureau releases data different geolevels, county, tract, block group, increasingly lower-level (higher-resolution). represent different frames observations might redistributed. useful observations one frame, want observations another. Frames may also roughly level (similar observation-group sizes), arranged differently (e.g., group individuals along different dimensions). case, might best redistribute data lower-level frame, back higher-level frame.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Redistribute Data","text":"Download R r-project.org, install package R console: load package:","code":"# install.packages(\"remotes\") remotes::install_github(\"uva-bi-sdad/redistribute\") library(redistribute)"},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 redistribute authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/download_census_pums.html","id":null,"dir":"Reference","previous_headings":"","what":"Download U.S. Census Microdata — download_census_pums","title":"Download U.S. Census Microdata — download_census_pums","text":"Download load U.S. census American Community Survey (ACS) Public Use Microdata Samples (PUMS): census.gov/programs-surveys/acs/microdata.html","code":""},{"path":"/reference/download_census_pums.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download U.S. Census Microdata — download_census_pums","text":"","code":"download_census_pums(dir, state, year = 2021, level = \"both\",   one_year = TRUE, calculate_error = FALSE, crosswalk = TRUE,   geoids = NULL, verbose = TRUE)"},{"path":"/reference/download_census_pums.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download U.S. Census Microdata — download_census_pums","text":"dir Directory save file(s). state Postal FIPS code state. year 4 digit year, 2005 recent year. level character indicating whether get person- household-level sample. Defaults . one_year Logical; FALSE, get 5-year estimates rather 1-year file. specified, fall back 5-year file 1-year file available. calculate_error Logical; TRUE, calculate standard errors variable using Successive Difference Replicate Formula census.gov/programs-surveys/acs/library/handbooks/pums.html. crosswalk Logical; FALSE, retrieve PUMA relationship files associating Census tracts PUM areas. treated TRUE geoids specified. geoids vector county, tract, block group GEOIDs within specified state select PUM areas ; defaults areas. specified, crosswalk treated TRUE. verbose Logical; FALSE, print status messages.","code":""},{"path":"/reference/download_census_pums.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download U.S. Census Microdata — download_census_pums","text":"list entries year state (specified), dictionary (containing data dictionary year), household /person (survey data), optionally household_error /person_error (calculate_error TRUE), crosswalk (crosswalk TRUE geoids specified).","code":""},{"path":"/reference/download_census_pums.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download U.S. Census Microdata — download_census_pums","text":"","code":"if (FALSE) { download_census_pums(\".\", \"va\") }"},{"path":"/reference/generate_population.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Population — generate_population","title":"Generate a Population — generate_population","text":"Simulate population individuals within households, complex relationships demographic location features.","code":""},{"path":"/reference/generate_population.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Population — generate_population","text":"","code":"generate_population(N = 1000, regions = NULL, capacities = NULL,   region_ids = NULL, attraction_loci = 3, random_regions = 0.1,   cost_loci = 2, size_loci = 5, similarity_metric = \"euclidean\",   n_neighbors = 50, neighbor_range = 0.5, n_races = 6,   n_building_types = 3, verbose = FALSE)"},{"path":"/reference/generate_population.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Population — generate_population","text":"N Number initial individuals generate. Final number individuals larger. regions vector region IDs, matrix coordinates, sf object geometries coordinates can derived. specified (capacities specified), regions similar housing units (mix single multi-family locations) generated. capacities vector maximum number households entry regions. region_ids vector unique IDs regions, column name regions containing IDs. attraction_loci Number locations selected centers attractiveness, influence households located. random_regions number 0 1, determines proportion people randomly relocated, case capacity households. cost_loci Number locations selected centers cost, influences initial income associated households. size_loci Number locations selected centers size, influence household sizes. similarity_metric Name metric use calculate nearness neighbors; see lma_simets. n_neighbors Number neighbors used influence new household's initial age race. neighbor_range Minimum similarity people considered neighbors, 0 1 (0 means unrestricted, 1 means region ). n_races Number different race groups sample . n_building_types Number different building types sample . verbose Logical; TRUE, show status messages.","code":""},{"path":"/reference/generate_population.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Population — generate_population","text":"list entries params (initial settings), two data.frames: households   individuals","code":""},{"path":"/reference/generate_population.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a Population — generate_population","text":"population generated two steps: First, households generated placed within regions. Placement within regions determined total distance one regions selected attraction loci. coordinates provided, first randomly generated. households placed, household incomes (first member) generated based cost loci, used generate building types (types increasingly associated income) household size (based size loci, income, building type). Renting status generated based income building type: 60% chance income mean income, 20% otherwise, multiplied .8 building type selected renting type, .3 otherwise. Second, individuals generated household. generate individual, first, neighbors searched , based n_neighbors neighbor_range. neighbors summarized: average age income, tabulated race. affect first member household: age first drawn Beta distribution (shapes 1 2 renting 1.5 otherwise, multiplied 80) added 18, adjusted toward random value centered average neighbor age (floored Gaussian standard deviation 1), race sampled (highest result Binomial draw n_races trials proportion neighbors * base rate chance success race group). Neighbors also affect income second member household first member's income neighbor mean income (40,000 given neighbors); case, second member's income drawn Gaussian distribution centered first member's income, standard deviation 10,000. second member's age based first member; floored Gaussian centered first member's age, standard deviation 15 first member's age 40, 5 otherwise, trimmed 18 90. second member's race 70% chance first member, 30% chance selected like first member's. Members second income, age randomly selected uniform distribution 0 first member's age minus 15 (rounded ), race determined either first second member (50% chance). Sex 50% chance 0 1 second member; sex 10% chance first member's, 90% chance opposite.","code":""},{"path":"/reference/generate_population.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a Population — generate_population","text":"","code":"generate_population(2) #> $params #> $params$neighbors #> [1] 50 #>  #> $params$range #> [1] 0.5 #>  #> $params$races_rates #> [1] 0.4731305 0.1186169 0.2704882 0.5000000 0.1430624 0.1530886 #>  #> $params$n_building_types #> [1] 3 #>  #>  #> $regions #>   id capacity     cost building_type      X      Y #> 1  1        1 474876.1             2  98488 108252 #> 2  2        1 481347.3             2 108290 106556 #>  #> $households #>   household region head_income size building_type renting #> 1         1      1      187712   11             2       1 #> 2         2      2      192513   10             2       1 #>  #> $individuals #>    household person neighbors age sex race income #> 1          1      1         0  27   0    0 187712 #> 2          1      2         0  31   1    0      0 #> 3          1      3         0   5   0    0      0 #> 4          1      4         0   7   0    0      0 #> 5          1      5         0   5   0    0      0 #> 6          1      6         0   4   0    0      0 #> 7          1      7         0   1   1    0      0 #> 8          1      8         0   5   0    0      0 #> 9          1      9         0   5   0    0      0 #> 10         1     10         0   8   0    0      0 #> 11         1     11         0   6   0    0      0 #> 12         2     12         0  38   0    0 192513 #> 13         2     13         0  34   1    0      0 #> 14         2     14         0   1   0    0      0 #> 15         2     15         0  11   0    0      0 #> 16         2     16         0   7   0    0      0 #> 17         2     17         0  12   1    0      0 #> 18         2     18         0  11   0    0      0 #> 19         2     19         0  12   0    0      0 #> 20         2     20         0   8   0    0      0 #> 21         2     21         0   3   0    0      0 #>"},{"path":"/reference/make_parent_polygons.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Higher-Order Polygons — make_parent_polygons","title":"Make Higher-Order Polygons — make_parent_polygons","text":"Randomly combine given polygons contiguous, larger polygons.","code":""},{"path":"/reference/make_parent_polygons.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Higher-Order Polygons — make_parent_polygons","text":"","code":"make_parent_polygons(polygons, ids = NULL, n = 2, strict_n = TRUE,   n_as_min = FALSE, buffer_dist = 5e-05, min_overlap = NULL,   verbose = TRUE)"},{"path":"/reference/make_parent_polygons.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Higher-Order Polygons — make_parent_polygons","text":"polygons sf object polygons reformed. ids vector IDs length polygons, column name polygons use IDs. n Number polygons aim combine new polygon. strict_n Logical; TRUE (default), n represents number intersecting polygons select, depending availability (minimum 2). FALSE, n represents number nearest polygons (centroid) use calculating box use selecting polygons (minimum 1). n_as_min Logical; TRUE, merge parents fewer n children random neighbor. Otherwise (default), parents may fewer n children. Applies strict_n TRUE. buffer_dist Distance around initial shape set shapes, used define neighboring shapes. Applies strict_n TRUE min_overlap Minimal area overlap potential neighboring shapes buffered target shape, used define neighboring shapes. Applies strict_n TRUE verbose Logical; FALSE, print status messages.","code":""},{"path":"/reference/make_parent_polygons.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Higher-Order Polygons — make_parent_polygons","text":"list entries new (sf object containing new polygons) map (list mapping old new polygon indices).","code":""},{"path":"/reference/redistribute.html","id":null,"dir":"Reference","previous_headings":"","what":"Redistribute Data — redistribute","title":"Redistribute Data — redistribute","text":"Distribute data source frame target frame.","code":""},{"path":"/reference/redistribute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Redistribute Data — redistribute","text":"","code":"redistribute(source, target = NULL, map = list(), source_id = \"GEOID\",   target_id = source_id, weight = NULL, source_variable = NULL,   source_value = NULL, aggregate = NULL, weight_agg_method = \"auto\",   default_value = NA, outFile = NULL, overwrite = FALSE,   make_intersect_map = FALSE, overlaps = \"keep\", use_all = TRUE,   return_geometry = TRUE, return_map = FALSE, verbose = FALSE)"},{"path":"/reference/redistribute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Redistribute Data — redistribute","text":"source matrix-like object want distribute ; usually real complete dataset, often lower resolution / higher level. target matrix-like object want distribute : usually dataset want available, often higher resolution / lower level (disaggregation). Can also single number, representing number initial characters source IDs derive target IDs (useful aggregating nested groups). map list entries named source IDs (aligning IDs), containing vectors associated target IDs (indices IDs). Entries can also numeric vectors IDs names, used weigh relationship. IDs related substrings (first characters target IDs source IDs), map can automatically generated . source target contain sf geometries, map made st_intersects (st_intersects(source, target)). intersects map made, source aggregated target, map entries contain multiple target IDs, entries weighted proportion overlap source area. source_id, target_id Name column source / target, vector containing IDs. source, default first column. target, columns searched one appears relate source IDs, falling back first column. weight Name column, vector containing weights (single value apply cases), apply target disaggregating, source aggregating. Defaults unit weights (weights 1). source_variable, source_value source tall (variables spread across rows rather columns), specifies names columns source containing variable names values conversion. aggregate Logical; specified, determine whether aggregate disaggregate source target. Otherwise, TRUE source observations target observations. weight_agg_method Means aggregating weight, case target IDs contain duplicates. Options \"sum\", \"average\", \"auto\" (default; sum weight integer-like, average otherwise). default_value Value set unmapped target ID. outFile Path CSV file save results. overwrite Logical; TRUE, overwrite existing outFile. make_intersect_map Logical; TRUE, opt calculate intersect-based map rather ID-based map, seem possible. specified FALSE, never calculate intersect-based map. overlaps specified TRUE \"keep\", assign target entities mapped multiple source entities single source entity. value determines entities weight assigned, \"first\" (default), \"last\", \"random\". use_all Logical; TRUE (default), redistribute map weights sum 1. Otherwise, entities may partially weighted. return_geometry Logical; FALSE, set returned data.frame's geometry target, exists. return_map Logical; TRUE, return map, without performing redistribution. Useful want inspect automatically created map, use later call. verbose Logical; TRUE, show status messages.","code":""},{"path":"/reference/redistribute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Redistribute Data — redistribute","text":"data.frame row target_ids (identified first column, id), column variable source.","code":""},{"path":"/reference/redistribute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Redistribute Data — redistribute","text":"","code":"# minimal example source <- data.frame(a = 1, b = 2) target <- 1:5 (redistribute(source, target, verbose = TRUE)) #> ℹ source IDs: 1 #> ℹ target IDs: `target` vector #> ℹ map: all target IDs for single source #> ℹ weights: 1 #> ℹ redistributing 2 variables from 1 source to 5 targets: #> • (numb; 2) a, b #> ℹ disaggregating... #> ✔ done disaggregating [8ms] #>  #>   id   a   b #> 1  1 0.2 0.4 #> 2  2 0.2 0.4 #> 3  3 0.2 0.4 #> 4  4 0.2 0.4 #> 5  5 0.2 0.4  # multi-entity example source <- data.frame(id = c(\"a\", \"b\"), cat = c(\"aaa\", \"bbb\"), num = c(1, 2)) target <- data.frame(   id = sample(paste0(c(\"a\", \"b\"), rep(1:5, 2))),   population = sample.int(1e5, 10) ) (redistribute(source, target, verbose = TRUE)) #> ℹ source IDs: id column of `source` #> ℹ target IDs: id column of `target` #> ℹ map: first 1 character of target IDs #> ℹ weights: 1 #> ℹ redistributing 2 variables from 2 sources to 10 targets: #> • (numb; 1) num #> • (char; 1) cat #> ℹ disaggregating... #> ✔ done disaggregating [11ms] #>  #> ℹ re-converting categorical levels #>    id cat num #> 1  a5 aaa 0.2 #> 2  b2 bbb 0.4 #> 3  a1 aaa 0.2 #> 4  b1 bbb 0.4 #> 5  b5 bbb 0.4 #> 6  b4 bbb 0.4 #> 7  a4 aaa 0.2 #> 8  b3 bbb 0.4 #> 9  a2 aaa 0.2 #> 10 a3 aaa 0.2"},{"path":"/reference/redistribute_parcel_pums_adj.html","id":null,"dir":"Reference","previous_headings":"","what":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"wrapper around redistribute function, takes PUMS data additional input, uses calculates adjusted weight redistribution parcel data.","code":""},{"path":"/reference/redistribute_parcel_pums_adj.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"","code":"redistribute_parcel_pums_adj(source, target, households,   target_total = \"Total_Units\", target_indicator = \"Unit_Type\",   households_size = \"size\", households_indicator = \"BLD\",   households_id = \"SERIALNO\", person = NULL,   person_household_id = \"SERIALNO\", ...)"},{"path":"/reference/redistribute_parcel_pums_adj.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"source, target Source target redistribute. households PUMS household data. Can list household person entries, like returned download_census_pums. target_total Column name target (e.g., parcel data; equivalent vector) containing unit count target_indicator Column name logical variable target (equivalent vector) use assign adjustment values, TRUE indicates single family home. values column characters, anything matching \"MULTI\" TRUE. households_size vector household sizes, column households containing values. specified found, person used calculate household size, based hosuehold_id person_household_id. households_indicator target_indicator, households, vector (column name) BLD codes, either \"02\" \"03\" TRUE. households_id vector household IDs aligning households, column name households containing IDs. used calculate households_size necessary. person PUMS person data. used calculate households_size necessary. person_household_id vector household IDs aligning person, column name person containing IDs. used calculate households_size necessary. ... Additional arguments pass redistribute. likely need specify map, potentially source_id /target_id.","code":""},{"path":"/reference/redistribute_parcel_pums_adj.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"Result redistribute. assumed parcel-level estimates, aggregated make higher-level estimates.","code":""},{"path":"/reference/redistribute_parcel_pums_adj.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"assumed initial weights unit counts, adjusted based PUMS household data.","code":""},{"path":"/reference/redistribute_parcel_pums_adj.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Method: Proportional, resident-estimate-adjusted parcel unit counts — redistribute_parcel_pums_adj","text":"","code":"if (FALSE) { if (require(\"tidycensus\")) {   # download source, target, and household data   tracts <- tidycensus::get_acs(     year = 2021,     state = \"51\",     county = \"013\",     geography = \"tract\",     output = \"wide\",     variables = c(total = \"B01001_001\"),     geometry = TRUE   )   parcels <- sf::st_read(paste0(     \"https://arlgis.arlingtonva.us/arcgis/rest/\",     \"services/Open_Data/od_MHUD_Polygons/\",     \"FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=json\"   ))   pums <- download_census_pums(tempdir(), \"51\", geoids = tracts$GEOID)    # calculate map between source and target   map_tr_to_parcel <- redistribute(     tracts, parcels,     target_id = \"OBJECTID\", return_map = TRUE   )    # redistribute tract-level summaries to parcels,   # using a resident-estimate-adjusted weight   parcels_filled <- redistribute_parcel_pums_adj(     tracts[, -2], parcels, pums,     map = map_tr_to_parcel, target_id = \"OBJECTID\"   )    # this can also be calculated with the underlying function    ## calculate resident estimates   household_size <- tapply(     table(pums$person$SERIALNO)[pums$household$SERIALNO],     pums$household$BLD %in% c(\"02\", \"03\"),     mean,     na.rm = TRUE   )   residents <- parcels$Total_Units * household_size[     as.character(parcels$Unit_Type != \"MULTI\")   ]    ## supply as weights   parcels_filled_manual <- redistribute(     tracts[, -2], parcels, map_tr_to_parcel,     target_id = \"OBJECTID\", weight = residents   )    # either way, you could now use the result to make estimates   # at higher-level geographies by redistributing from the   # parcel-level estimates to the new target } }"}]
