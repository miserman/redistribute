[{"path":"/articles/estimate_comparisons.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Estimate Comparisons","text":"’ll focus single county (Arlington, VA), census block groups Public Use Microdata Samples (PUMS) primary source data. Ultimately, ’ll want use methods get values unobserved geographies, comparison methods, ’ll want way test results known values. test case, can simply start higher geolayer, make estimates block group level. , ’ll need aggregate block group data tracts:","code":"# directory to save data in base_dir <- \"../estimate_comparison\" dir.create(base_dir, FALSE) # geographies library(catchment) geography_bg <- download_census_shapes(base_dir, \"va\", \"bg\", year = 2021) geography_bg <- geography_bg[grep(\"^51013\", geography_bg$GEOID), ] geography_tr <- download_census_shapes(base_dir, \"va\", \"tr\", year = 2021) geography_tr <- geography_tr[grep(\"^51013\", geography_tr$GEOID), ]  # ACS data  ## block groups block_groups <- tidycensus::get_acs(   year = 2021,   state = \"51\",   county = \"013\",   geography = \"block group\",   output = \"wide\",   variables = c(     total = \"B01001_001\",     race_white = \"B02001_002\",     race_black = \"B02001_003\",     race_native = \"B02001_004\",     race_asian = \"B02001_005\",     sex_male = \"B01001_002\",     sex_female = \"B01001_026\",     tenure_total = \"B25003_001\",     tenure_owner = \"B25003_002\",     tenure_renter = \"B25003_003\",     income_total = \"B19001_001\",     income_lt10 = \"B19001_002\",     income_10_15 = \"B19001_003\",     income_15_20 = \"B19001_004\",     income_20_25 = \"B19001_005\",     income_25_30 = \"B19001_006\",     income_30_35 = \"B19001_007\",     income_35_40 = \"B19001_008\",     income_40_45 = \"B19001_009\",     income_45_50 = \"B19001_010\",     income_100_125 = \"B19001_014\",     income_125_150 = \"B19001_015\",     income_150_200 = \"B19001_016\",     income_gt200 = \"B19001_017\"   ) )[, -2] #> Getting data from the 2017-2021 5-year ACS colnames(block_groups)[2] <- \"Total\" block_groups$race_other <- block_groups$Total - rowSums(   block_groups[, grep(\"^race_.*E$\", colnames(block_groups))] ) colnames(block_groups) <- sub(\"E$\", \"\", colnames(block_groups))  library(sf) rownames(geography_bg) <- geography_bg$GEOID st_geometry(block_groups) <- st_geometry(geography_bg[block_groups$GEOID, ])  # parcel data parcel_file <- paste0(base_dir, \"/parcels.rds\") if (!file.exists(parcel_file)) {   parcels <- st_read(paste0(     \"https://arlgis.arlingtonva.us/arcgis/rest/services/Open_Data/od_MHUD_Polygons/\",     \"FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=json\"   ))   saveRDS(parcels, parcel_file, compress = \"xz\") } parcels <- readRDS(parcel_file) colnames(parcels)[colnames(parcels) == \"Total_Units\"] <- \"Total\" parcels$tract <- substring(parcels$Full_Block, 1, 11) parcels <- parcels[parcels$Year_Built <= 2021, ] tracts <- redistribute(block_groups, geography_tr)"},{"path":"/articles/estimate_comparisons.html","id":"redistribution","dir":"Articles","previous_headings":"","what":"Redistribution","title":"Estimate Comparisons","text":"baseline, first method consider proportional redistribution block group summary estimates alone.","code":""},{"path":"/articles/estimate_comparisons.html","id":"direct","dir":"Articles","previous_headings":"Redistribution","what":"Direct","title":"Estimate Comparisons","text":"tracts block groups directly: can see case, error introduce proportional information lower level; block groups varied within tract now value:","code":"# going from tracts to block groups with no information estimate_tr_to_bg <- redistribute(   tracts, block_groups,   source_id = \"id\" )  # mean absolute error between total population estimates mean(abs(estimate_tr_to_bg$Total - block_groups$Total)) #> [1] 319.8405"},{"path":"/articles/estimate_comparisons.html","id":"down-and-up","dir":"Articles","previous_headings":"Redistribution","what":"Down and Up","title":"Estimate Comparisons","text":"go tracts parcels, back block groups: Now original variation block groups:  parcel totals living units rather individuals, may able refine redistribution based heuristic estimates (tweaking) number people per living unit:","code":"# function to apply tract to parcel to block group redistribution redist_downup <- function(     top, bottom, middle, map_tb = NULL, map_bm = NULL,     weight = \"Total\", source_id = \"id\",     target_id = \"OBJECTID\", target_id_mid = \"GEOID\") {   to_bottom <- redistribute(     top, bottom, map_tb,     weight = weight, source_id = source_id, target_id = target_id,     default_value = 0   )   redistribute(     to_bottom, middle, map_bm,     source_id = \"id\", target_id = target_id_mid,     default_value = 0   ) }  # pre-make maps  ## going from tracts to parcels map_tr_to_parcel <- redistribute(   tracts, parcels,   source_id = \"id\", target_id = \"OBJECTID\", return_map = TRUE ) ## and from parcels to block groups map_parcel_to_bg <- redistribute(   parcels, block_groups,   source_id = \"OBJECTID\", target_id = \"GEOID\", return_map = TRUE )  # redistribute estimate_redist <- redist_downup(   tracts, parcels, block_groups, map_tr_to_parcel, map_parcel_to_bg )  # mean absolute error between total population estimates mean(abs(estimate_redist$Total - block_groups$Total)) #> [1] 221.8834 # start with number of units parcels$Residents <- parcels$Total  ## for parcels with more than 1 unit, assume 2.255 per unit su <- parcels$Total > 1 parcels$Residents[su] <- parcels$Total[su] * 2.255  ## for 1-unit parcels, base on size su <- parcels$Total == 1 parcels$Residents[su] <- as.numeric(cut(   parcels$Shape__Area[su], c(-Inf, 9170, 9550, 48000, Inf) )) + 4 parcels$Residents[su & parcels$Residents > 7] <- 137.7  # now see if this improved our redistribution estimate_redist_adj <- redist_downup(   tracts, parcels, block_groups, map_tr_to_parcel, map_parcel_to_bg,   weight = \"Residents\" )  # mean absolute error between total population estimates mean(abs(estimate_redist_adj$Total - block_groups$Total)) #> [1] 180.9204"},{"path":"/articles/estimate_comparisons.html","id":"estimation","dir":"Articles","previous_headings":"","what":"Estimation","title":"Estimate Comparisons","text":"far, ’ve used additional information (total units) parcel level improve distribution values across block groups, relative uniform distribution. information parcel level, unit type number renter versus owner designations, make use information, need association source values features. census microdata sample can come : Public Use Microdata Sample (PUMS) consists individual-level observations include demographic features housing features. PUM sample located PUM areas, made tracts, can start baseline. use associations demographic housing features within PUMA get probabilities parcel level, redistribute source values like . first step end prepare data easy associate individual PUMS variables per-level summaries tract level: Now can apply method set data.","code":"pums <- download_census_pums(base_dir, \"va\", 2021) #> ℹ loading household sample: h1-Year.csv.xz #> ℹ loading person sample: p1-Year.csv.xz #> ℹ loading crosswalk 2020_Census_Tract_to_2020_PUMA.txt  # prepare IDs pums$crosswalk$GEOID <- do.call(paste0, pums$crosswalk[, 1:3]) pums$person$ID <- do.call(paste0, pums$person[, c(\"SERIALNO\", \"SPORDER\")])  # prepare variables  ## survey categories pums$person$sex_cat <- c(\"sex_male\", \"sex_female\")[as.numeric(pums$person$SEX)]  pums$person$race_cat <- paste0(\"race_\", c(   \"white\", \"black\", rep(\"native\", 3), \"asian\", rep(\"other\", 3) ))[as.numeric(pums$person$RAC1P)]  income_cols <- grep(\"income_\\\\d[_0-9]+$\", colnames(tracts)) pums$household$income_cat <- as.character(cut(pums$household$HINCP, c(   -Inf, as.integer(gsub(\"^income_|_[0-9]+$\", \"\", colnames(tracts)[income_cols])),   200, Inf ) * 1e3, c(\"income_lt10\", colnames(tracts)[income_cols], \"income_gt200\")))  ## unit categories pums$household$building_type <- \"OTHER\" pums$household$building_type[pums$household$BLD == \"02\"] <- \"SFD\" pums$household$building_type[pums$household$BLD == \"03\"] <- \"SFA\" pums$household$building_type[   is.na(pums$household$BLD) | pums$household$BLD %in% paste0(\"0\", 4:9) ] <- \"MULTI\" pums$household <- pums$household[pums$household$building_type != \"OTHER\", ]  pums$household$status <- \"OTHER\" pums$household$status[pums$household$TEN %in% 1:2] <- \"OWNER\" pums$household$status[pums$household$TEN == 3] <- \"RENTER\" pums$household <- pums$household[pums$household$status != \"OTHER\", ] # define variables of interest vars_house <- c(   ID = \"SERIALNO\", weight = \"WGTP\", type = \"building_type\",   status = \"status\", income = \"income_cat\" ) vars_person <- c(   ID = \"ID\", weight = \"PWGTP\", sex_cat = \"sex_cat\", race_cat = \"race_cat\" ) vars_units <- c(   type = \"Unit_Type\", renters = \"Renter_Occupied\", owners = \"Owner_Occupied\" )  ## get their levels vars_list <- c(   lapply(vars_person[-(1:2)], function(var) unique(pums$person[[var]])),   income_cat = list(unique(pums$household$income_cat)) ) vars <- unlist(vars_list, use.names = FALSE) return_vars <- names(vars_list)  # prepare datasets split into PUMAs pumas_focal <- unique(pums$crosswalk$PUMA5CE[pums$crosswalk$GEOID %in% tracts$id]) data <- lapply(structure(pumas_focal, names = pumas_focal), function(puma) {   households <- pums$household[pums$household$PUMA == puma, vars_house]   ids <- pums$crosswalk$GEOID[     pums$crosswalk$PUMA5CE == puma & pums$crosswalk$GEOID %in% tracts$id   ]   list(     households = households,     individuals = do.call(rbind, lapply(seq_len(nrow(households)), function(r) {       h <- households[r, ]       d <- pums$person[pums$person$SERIALNO == h$SERIALNO, vars_person]       cbind(h[rep(1L, nrow(d)), ], d)     })),     source = tracts[tracts$id %in% ids, c(\"id\", vars), drop = TRUE],     parcels = parcels[       parcels$tract %in% ids, c(\"tract\", \"OBJECTID\", vars_units),       drop = TRUE     ]   ) })  # function to apply each method to the data apply_method <- function(data, method) {   p <- do.call(rbind, lapply(data, function(set) {     set$source <- st_drop_geometry(set$source)     do.call(rbind, lapply(unique(set$source$id), function(id) {       source <- set$source[set$source$id == id, -1]       target <- set$parcels[if (\"map\" %in% names(set)) {         set$parcels$tract %in% substring(set$map[[id]], 1, 11)       } else {         set$parcels$tract == id       }, ]       if (nrow(target)) {         # adjust resident totals to match source totals         vcols <- grep(\"Occupied\", colnames(target))         ptotals <- rowSums(target[, vcols])         target[, vcols] <- target[, vcols] / ptotals *           (sum(source[, 1:2]) * ptotals / sum(target[, vcols]))          est <- method(source, target[, -1], set$individuals)         su <- !vars %in% colnames(est)         if (any(su)) est[, vars[su]] <- 0         cbind(target[, 1:2], est[as.character(target$OBJECTID), vars])       }     }))   }))   list(parcels = p, block_groups = redistribute(     p[, -1], block_groups, map_parcel_to_bg,     source_id = \"OBJECTID\", target_id = \"GEOID\", default_value = 0   )) }"},{"path":"/articles/estimate_comparisons.html","id":"initial-weights","dir":"Articles","previous_headings":"Estimation","what":"Initial Weights","title":"Estimate Comparisons","text":"baseline, can calculate set proportions PUMA, apply parcels contained tract:","code":"# function to apply the method: # - source: the tract-level totals # - target: the parcels # - individuals: the individual PUM responses est_baseline <- function(source, target, individuals) {   do.call(rbind, lapply(unique(target$Unit_Type), function(type) {     d <- target[target$Unit_Type == type, ]     nd <- nrow(d)     colnames(d)[-(1:2)] <- c(\"RENTER\", \"OWNER\")     i <- individuals[       individuals$building_type == type, c(\"PWGTP\", \"status\", return_vars)     ]     as.data.frame(Reduce(\"+\", lapply(unique(i$status), function(s) {       ii <- i[i$status == s, ]       weight_total <- sum(ii$PWGTP)       do.call(cbind, lapply(return_vars, function(cat) {         props <- tapply(ii$PWGTP, ii[[cat]], sum) / weight_total         props[vars_list[[cat]][!vars_list[[cat]] %in% names(props)]] <- 0         props[is.na(props)] <- 0         matrix(           d[[s]] * rep(props, each = nd),           nd,           dimnames = list(d$OBJECTID, names(props))         )       }))     })))   })) }  # apply across PUMAs and tracts within them # and aggregate to block groups estimate_baseline <- apply_method(data, est_baseline)"},{"path":"/articles/estimate_comparisons.html","id":"raking","dir":"Articles","previous_headings":"Estimation","what":"Raking","title":"Estimate Comparisons","text":"baseline, ’re using PUMS weights calculate proportions, weights meant bring totals line PUMA-level estimates; Since lower-level information form tract-level totals, can adjust weights match smaller-area summary:","code":"library(anesrake) rake_prep <- function() {   eval(expression({     for (var in names(totals)) {       individuals[[var]] <- as.factor(individuals[[var]])       totals[[var]] <- totals[[var]][levels(individuals[[var]])]       su <- which(is.na(totals[[var]]) | totals[[var]] == 0)       if (length(su)) totals[[var]][su] <- 1       totals[[var]] <- totals[[var]] / sum(totals[[var]])     }     totals <- Filter(length, totals)   }), parent.frame()) } est_rake <- function(source, target, individuals) {   totals <- lapply(     structure(names(vars_list)[1:2], names = names(vars_list)[1:2]),     function(n) unlist(source[, vars_list[[n]]])   )   rake_prep()   individuals$status <- as.factor(individuals$status)   individuals$building_type <- as.factor(individuals$building_type)   individuals$income_cat <- as.factor(individuals$income_cat)   capture.output(individuals$weight <- tryCatch(     anesrake(       totals, individuals, individuals$ID, individuals$PWGTP,       pctlim = .001     )$weightvec[individuals$ID],     error = function(e) {       warning(e$message)       individuals$PWGTP     }   ))   # calculate proportions, and estimate parcel values   do.call(rbind, lapply(unique(target$Unit_Type), function(type) {     d <- target[target$Unit_Type == type, ]     nd <- nrow(d)     colnames(d)[-(1:2)] <- c(\"RENTER\", \"OWNER\")     i <- individuals[       individuals$building_type == type, c(\"weight\", \"status\", return_vars)     ]     as.data.frame(Reduce(\"+\", lapply(levels(i$status), function(s) {       ii <- i[i$status == s, ]       weight_total <- sum(ii$weight)       do.call(cbind, lapply(return_vars, function(cat) {         props <- tapply(ii$weight, ii[[cat]], sum) / weight_total         props[is.na(props)] <- 0         matrix(           d[[s]] * rep(props, each = nd),           nd,           dimnames = list(d$OBJECTID, names(props))         )       }))     })))   })) } estimate_rake <- apply_method(data, est_rake)"},{"path":"/articles/estimate_comparisons.html","id":"two-step-raking","dir":"Articles","previous_headings":"Estimation","what":"Two Step Raking","title":"Estimate Comparisons","text":"initial raking approach, considered person-level variables, also household-level information, might try account well first adjusting household weights, using adjusted weights initialize adjusted person-level weights:","code":"est_twostep <- function(source, target, individuals) {   # step 1: initial weights at household level   totals <- list(     building_type = tapply(rowSums(target[, -(1:2)]), target$Unit_Type, sum),     status = structure(colSums(target[, -(1:2)]), names = c(\"RENTER\", \"OWNER\")),     income_cat = unlist(source[, grep(\"^income_\", colnames(source))])   )   rake_prep()   capture.output(individuals$weight <- tryCatch(     anesrake(       totals, individuals, individuals$SERIALNO, individuals$WGTP,       pctlim = .001     )$weightvec[individuals$SERIALNO],     error = function(e) {       warning(e$message)       individuals$WGTP     }   ))   # step 2: adjust at person level   totals <- lapply(     structure(names(vars_list)[1:2], names = names(vars_list)[1:2]),     function(n) unlist(source[, vars_list[[n]]])   )   rake_prep()   capture.output(individuals$weight <- tryCatch(     anesrake(       totals, individuals, individuals$ID,       individuals$PWGTP * individuals$weight / individuals$WGTP,       pctlim = .001     )$weightvec[individuals$ID],     error = function(e) {       warning(e$message)       individuals$PWGTP * individuals$weight / individuals$WGTP     }   ))   # calculate proportions, and estimate parcel values   do.call(rbind, lapply(unique(target$Unit_Type), function(type) {     d <- target[target$Unit_Type == type, ]     nd <- nrow(d)     colnames(d)[-(1:2)] <- c(\"RENTER\", \"OWNER\")     i <- individuals[       individuals$building_type == type, c(\"weight\", \"status\", return_vars)     ]     as.data.frame(Reduce(\"+\", lapply(levels(i$status), function(s) {       ii <- i[i$status == s, ]       weight_total <- sum(ii$weight)       do.call(cbind, lapply(return_vars, function(cat) {         props <- tapply(ii$weight, ii[[cat]], sum) / weight_total         props[is.na(props)] <- 0         matrix(           d[[s]] * rep(props, each = nd),           nd,           dimnames = list(d$OBJECTID, names(props))         )       }))     })))   })) } estimate_twostep <- apply_method(data, est_twostep)"},{"path":"/articles/estimate_comparisons.html","id":"generalized-raking","dir":"Articles","previous_headings":"Estimation","what":"Generalized Raking","title":"Estimate Comparisons","text":"two-step approach still mainly focuses better aligning person-level weights – household totals match first step, considered second step. Alternative , might try match household- person-level weights time generalized raking approach:","code":"library(mlfit) est_grake <- function(source, target, individuals) {   person <- lapply(names(vars_list)[1:2], function(n) {     l <- vars_list[[n]]     s <- data.frame(level = l, count = as.numeric(source[, l, drop = TRUE]))     colnames(s)[1] <- n     s   })   total <- sum(target[, -(1:2)])   household <- unique(individuals$building_type)   household <- structure(numeric(length(household)), names = household)   observed_types <- tapply(rowSums(target[, -(1:2)]), target$Unit_Type, sum)   household[names(observed_types)] <- observed_types   household <- list(data.frame(building_type = names(household), count = household))   household[[1]]$count[is.na(household[[1]]$count)] <- 1   household[[2]] <- data.frame(     status = c(\"RENTER\", \"OWNER\"), count = colSums(target[, -(1:2)])   )   household[[3]] <- unlist(source[, grep(\"^income_\", colnames(source))])   household[[3]] <- data.frame(     income_cat = names(household[[3]]), count = household[[3]]   )   names(household) <- c(\"building_type\", \"status\", \"income_cat\")   household <- lapply(names(household), function(var) {     l <- household[[var]]     l <- l[l[[1]] %in% unique(individuals[, var]), ]     l[[2]] <- l[[2]] / sum(l[[2]]) * total     l   })   m <- ml_problem(     individuals,     field_names = special_field_names(\"SERIALNO\", \"ID\", count = \"count\"),     group_controls = household,     individual_controls = person,     prior_weights = individuals$WGTP   )   individuals$weight <- tryCatch(     suppressWarnings(ml_fit_dss(m, ginv = solve)$weights),     error = function(e) {       warning(e$message)       NULL     }   )   if (is.null(individuals$weight)) {     individuals$weight <- tryCatch(       suppressWarnings(ml_fit_ipu(m)$weights),       error = function(e) {         warning(e$message)         individuals$PWGTP       }     )   }   do.call(rbind, lapply(unique(target$Unit_Type), function(type) {     d <- target[target$Unit_Type == type, ]     nd <- nrow(d)     colnames(d)[-(1:2)] <- c(\"RENTER\", \"OWNER\")     i <- individuals[       individuals$building_type == type, c(\"weight\", \"status\", return_vars)     ]     as.data.frame(Reduce(\"+\", lapply(unique(i$status), function(s) {       ii <- i[i$status == s, ]       weight_total <- sum(ii$weight)       do.call(cbind, lapply(return_vars, function(cat) {         props <- tapply(ii$weight, ii[[cat]], sum) / weight_total         props[vars_list[[cat]][!vars_list[[cat]] %in% names(props)]] <- 0         props[is.na(props)] <- 0         matrix(           d[[s]] * rep(props, each = nd),           nd,           dimnames = list(d$OBJECTID, names(props))         )       }))     })))   })) } estimate_grake <- apply_method(data, est_grake)"},{"path":"/articles/estimate_comparisons.html","id":"comparison","dir":"Articles","previous_headings":"Estimation","what":"Comparison","title":"Estimate Comparisons","text":"Now can compare considered methods within variables used inform raking approaches:","code":"error <- do.call(rbind, lapply(structure(vars, names = vars), function(var) {   d <- cbind(     Prop = estimate_redist[[var]],     \"Prop Adj\" = estimate_redist_adj[[var]],     Baseline = estimate_baseline$block_groups[[var]],     Rake = estimate_rake$block_groups[[var]],     \"Rake 2 Step\" = estimate_twostep$block_groups[[var]],     \"Rake General\" = estimate_grake$block_groups[[var]]   )   colMeans(abs(d - block_groups[[var]])) })) error <- rbind(error, Average = colMeans(error)) kable(error, digits = 2)"},{"path":"/articles/estimate_comparisons.html","id":"alternate-geolayers","dir":"Articles","previous_headings":"Estimation","what":"Alternate Geolayers","title":"Estimate Comparisons","text":"far, ’ve used geolayer-related test-case assess methods, may effect . alternative, might abandon canonical tracts favor randomly agglutinated block groups. can also take look virtual tracts created run: Since virtual tracts randomly created, might also look results across runs sense variation:","code":"# randomly combine adjacent block groups into pairs of 2 (were possible) # apply within each PUMA apply_altgeo <- function(run = NULL, return_full = FALSE) {   vdata <- lapply(structure(pumas_focal, names = pumas_focal), function(puma) {     d <- data[[puma]]     su <- substring(block_groups$GEOID, 1, 11) %in% d$source$id     vtr <- make_parent_polygons(       block_groups[su, ], \"GEOID\",       n_as_min = TRUE, verbose = FALSE     )     names(vtr$new) <- names(vtr$map) <- paste0(puma, \"_\", seq_along(vtr$new))     d$geography <- vtr$new     d$map <- vtr$map     d$source <- redistribute(       block_groups[su, c(\"GEOID\", vars)], vtr$new, vtr$map,       default_value = 0     )     d   })   vsource <- do.call(rbind, lapply(vdata, \"[[\", \"source\"))   vmap <- redistribute(     vsource, parcels,     source_id = \"id\", target_id = \"OBJECTID\", return_map = TRUE   )   vestimates <- list(     Prop = list(block_groups = redist_downup(       vsource, parcels, block_groups, vmap, map_parcel_to_bg     )),     \"Prop Adj\" = list(block_groups = redist_downup(       vsource, parcels, block_groups, vmap, map_parcel_to_bg,       weight = \"Residents\"     )),     Baseline = apply_method(vdata, est_baseline),     Rake = apply_method(vdata, est_rake),     \"Rake 2 Step\" = apply_method(vdata, est_twostep),     \"Rake General\" = apply_method(vdata, est_grake)   )   verror <- do.call(rbind, lapply(structure(vars, names = vars), function(var) {     colMeans(abs(as.data.frame(       lapply(vestimates, function(d) d$block_groups[[var]]),       check.names = FALSE     ) - block_groups[[var]]))   }))   verror <- rbind(verror, Average = colMeans(verror))   if (return_full) {     list(       data = data, source = vsource, map = vmap,       estimates = vestimates, error = verror     )   } else {     verror   } }  estimates_altgeo <- apply_altgeo(return_full = TRUE) kable(estimates_altgeo$error, digits = 2) # since this can be quite long-running, # we'll save and reload results results_altgeo_file <- paste0(base_dir, \"/results_altgeo.rds\") if (file.exists(results_altgeo_file)) {   results_altgeo <- readRDS(results_altgeo_file) } else {   results_altgeo <- lapply(1:50, apply_altgeo)   saveRDS(results_altgeo, results_altgeo_file) }  kable(Reduce(\"+\", results_altgeo) / length(results_altgeo), digits = 2)"},{"path":"/articles/introduction.html","id":"basic-example","dir":"Articles","previous_headings":"","what":"Basic Example","title":"Introduction to Redistribution","text":"Say 1 set 5 regions:","code":"regions <- data.frame(id = 1:5) regions #>   id #> 1  1 #> 2  2 #> 3  3 #> 4  4 #> 5  5"},{"path":"/articles/introduction.html","id":"disaggregate","dir":"Articles","previous_headings":"Basic Example","what":"Disaggregate","title":"Introduction to Redistribution","text":"single observation entire set, disaggregate regions. additional information, best guess value region proportional split – case, one fifth observed value region: , maybe know little regions, size; use information adjust proportion allotted region:","code":"# install if needed: remotes::install_github(\"uva-bi-sdad/redistribute\") library(redistribute)  set_value <- 1 (redistribute(set_value, regions)) #>   id  V1 #> 1  1 0.2 #> 2  2 0.2 #> 3  3 0.2 #> 4  4 0.2 #> 5  5 0.2 region_values <- redistribute(   set_value, regions,   weight = c(1, 10, 10, 20, 50) ) region_values #>   id         V1 #> 1  1 0.01098901 #> 2  2 0.10989011 #> 3  3 0.10989011 #> 4  4 0.21978022 #> 5  5 0.54945055"},{"path":"/articles/introduction.html","id":"aggregate","dir":"Articles","previous_headings":"Basic Example","what":"Aggregate","title":"Introduction to Redistribution","text":"observations regions, aggregate single value set. case, can re-aggregate initially disaggregated, recover original observation:","code":"(redistribute(region_values)) #>   id V1 #> 1  1  1"},{"path":"/articles/introduction.html","id":"applied-example","dir":"Articles","previous_headings":"","what":"Applied Example","title":"Introduction to Redistribution","text":"One use case redistribution converting demographics data geographic layers. illustration, can look U.S. Census data Fairfax, Virginia: population information provided Census Block Group level lowest, might want look population within zip codes. try aggregating directly block groups zip codes, involves calculating proportional intersections region polygons: also disaggregate parcel level, point locations, aggregate zipcodes: Now can compare estimated total population different methods provided:","code":"base_dir <- \"~/Downloads\" # remotes::install_github(\"uva-bi-sdad/catchment\") library(catchment) library(sf)  # download population and data shapes population <- download_census_population(base_dir, \"VA\", 2020)$estimates #> ℹ loading existing Virginia population data population <- population[grepl(\"^51(?:059|600)\", population$GEOID), ] population[, -1] <- vapply(   population[, -1], as.numeric, numeric(nrow(population)) ) rownames(population) <- population$GEOID block_groups <- st_transform(   download_census_shapes(base_dir, \"VA\", \"bg\", year = 2020), \"WGS84\" ) block_groups <- block_groups[block_groups$GEOID %in% population$GEOID, ] population <- population[block_groups$GEOID, ] st_geometry(population) <- block_groups$geometry population$Overall <- population$TOTAL.POPULATION_Total  # prepare a map library(leaflet) pal <- colorNumeric(scico::scico(255, palette = \"lajolla\"), population$Overall) map <- leaflet(   population,   options = leafletOptions(attributionControl = FALSE) ) |>   addProviderTiles(\"CartoDB.Positron\") |>   addScaleBar(\"bottomleft\") |>   addControl(\"Total Population\", \"topright\") |>   addLayersControl(     position = \"topleft\", overlayGroups = \"Block Groups\",     baseGroups = c(       \"Zip Codes\", \"Parcels -> Zip Codes\", \"Block Groups -> Zip Codes\",       \"Block Groups -> Parcels -> Zip Codes\"     )   ) |>   addLegend(     \"bottomright\", pal, ~Overall,     title = \"Block Groups\", opacity = 1   ) |>   addPolygons(     fillColor = ~ pal(Overall), fillOpacity = 1, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Block Groups\", label = ~ paste(       GEOID, \"Population:\", round(Overall, 3)     )   ) # Download shapes if needed zipcode_file <- paste0(base_dir, \"/zipcode_va_fairfax.geojson\") if (!file.exists(zipcode_file)) {   download.file(paste0(     \"https://www.fairfaxcounty.gov/euclid/rest/services/IPLS/IPLSMap\",     \"/MapServer/3/query?where=1=1&outFields=*&outSR=4326&f=geojson\"   ), zipcode_file) } zipcodes <- read_sf(zipcode_file)  # redistribute population data from block groups zipcode_population <- redistribute(   population, zipcodes,   target_id = \"ZIPCODE\" ) # Download shapes if needed ## https://data-fairfaxcountygis.opendata.arcgis.com/datasets/current-population parcel_file <- paste0(base_dir, \"/parcel_va_fairfax.geojson\") if (!file.exists(parcel_file)) {   download.file(paste0(     \"https://opendata.arcgis.com/api/v3/datasets/\",     \"314bfe4019754952a715be3a33384d9d_0/downloads/data\",     \"?format=geojson&spatialRefId=4326&where=1=1\"   ), parcel_file) } parcels <- read_sf(parcel_file)  # disaggregate population data from block groups to parcels bg_parcel_population <- redistribute(   population, parcels,   weight = \"CURRE_POPUL\" )  # then aggregate from parcels to zip codes bg_parcel_zipcode_population <- redistribute(   bg_parcel_population, zipcodes,   source_id = \"id\", target_id = \"ZIPCODE\" )  # since it's provided in this case, we can also just aggregate # up from parcels directly parcel_zipcode_population <- redistribute(   parcels, zipcodes,   source_id = \"PIN\", target_id = \"ZIPCODE\" ) all_values <- c(   zipcodes$POPULATION, zipcode_population$Overall,   bg_parcel_zipcode_population$Overall, parcel_zipcode_population$CURRE_POPUL ) pal_zip <- colorNumeric(scico::scico(255, palette = \"lajolla\"), all_values) map |>   addPolygons(     data = zipcodes,     fillColor = ~ pal_zip(POPULATION), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Zip Codes\", label = ~ paste(       ZIPCODE, \"Population:\", round(POPULATION, 3)     )   ) |>   hideGroup(\"Zip Codes\") |>   addPolygons(     data = parcel_zipcode_population,     fillColor = ~ pal_zip(CURRE_POPUL), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Parcels -> Zip Codes\", label = ~ paste(       id, \"Population:\", round(CURRE_POPUL, 3)     )   ) |>   hideGroup(\"Parcels -> Zip Codes\") |>   addPolygons(     data = zipcode_population,     fillColor = ~ pal_zip(Overall), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Block Groups -> Zip Codes\", label = ~ paste(       id, \"Population:\", round(Overall, 3)     )   ) |>   hideGroup(\"Block Groups -> Zip Codes\") |>   addPolygons(     data = bg_parcel_zipcode_population,     fillColor = ~ pal_zip(Overall), fillOpacity = .8, weight = 1,     color = \"#000\", highlightOptions = highlightOptions(color = \"#fff\"),     group = \"Block Groups -> Parcels -> Zip Codes\", label = ~ paste(       id, \"Population:\", round(Overall, 3)     )   ) |>   showGroup(\"Block Groups -> Parcels -> Zip Codes\") |>   addLegend(\"bottomright\", pal_zip, all_values, opacity = 1, title = \"Zip Codes\")"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Biocomplexity Institute. Copyright holder, funder. Micah Iserman. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Iserman M (2023). redistribute: Redistribute Data. R package version 0.0.1, https://github.com/uva-bi-sdad/redistribute.","code":"@Manual{,   title = {redistribute: Redistribute Data},   author = {Micah Iserman},   year = {2023},   note = {R package version 0.0.1},   url = {https://github.com/uva-bi-sdad/redistribute}, }"},{"path":"/index.html","id":"redistribute","dir":"","previous_headings":"","what":"Redistribute Data","title":"Redistribute Data","text":"R package redistribute data. data redistributed (source) observed given frame (across rows associated IDs). data redistributed new frames (target; different rows IDs mapped source). Generally, frames represent groupings, lowest-level frame contains single observations (e.g., individual, individual single time-point, time-point single source, etc.), highest-level frame single observation entire population. example, U.S. Census Bureau releases data different geolevels, county, tract, block group, increasingly lower-level (higher-resolution). represent different frames observations might redistributed. useful observations one frame, want observations another. Frames may also roughly level (similar observation-group sizes), arranged differently (e.g., group individuals along different dimensions). case, might best redistribute data lower-level frame, back higher-level frame.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Redistribute Data","text":"Download R r-project.org, install package R console: load package:","code":"# install.packages(\"remotes\") remotes::install_github(\"uva-bi-sdad/redistribute\") library(redistribute)"},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 redistribute authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/download_census_pums.html","id":null,"dir":"Reference","previous_headings":"","what":"Download U.S. Census Microdata — download_census_pums","title":"Download U.S. Census Microdata — download_census_pums","text":"Download load U.S. census American Community Survey (ACS) Public Use Microdata Samples (PUMS): (census.gov/programs-surveys/acs/microdata.html)[https://www.census.gov/programs-surveys/acs/microdata.html]","code":""},{"path":"/reference/download_census_pums.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download U.S. Census Microdata — download_census_pums","text":"","code":"download_census_pums(dir, state, year = 2021, level = \"both\",   one_year = TRUE, calculate_error = FALSE, crosswalk = TRUE,   geoids = NULL, verbose = TRUE)"},{"path":"/reference/download_census_pums.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download U.S. Census Microdata — download_census_pums","text":"dir Directory save file(s). state Postal FIPS code state. year 4 digit year, 2005 recent year. level character indicating whether get person- household-level sample. Defaults . one_year Logical; FALSE, get 5-year estimates rather 1-year file. specified, fall back 5-year file 1-year file available. calculate_error Logical; TRUE, calculate standard errors variable using Successive Difference Replicate Formula [PUMS handbook](https://www.census.gov/programs-surveys/acs/library/handbooks/pums.html). crosswalk Logical; FALSE, retrieve PUMA relationship files associating Census tracts PUM areas. treated TRUE geoids specified. geoids vector county, tract, block group GEOIDs within specified state select PUM areas ; defaults areas. specified, crosswalk treated TRUE. verbose Logical; FALSE, print status messages.","code":""},{"path":"/reference/download_census_pums.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download U.S. Census Microdata — download_census_pums","text":"list entries year state (specified), dictionary (containing data dictionary year), household /person (survey data), optionally household_error /person_error (calculate_error TRUE), crosswalk (crosswalk TRUE geoids specified).","code":""},{"path":"/reference/download_census_pums.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download U.S. Census Microdata — download_census_pums","text":"","code":"if (FALSE) { download_census_pums(\".\", \"va\") }"},{"path":"/reference/generate_population.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Population — generate_population","title":"Generate a Population — generate_population","text":"Simulate population individuals within households, complex relationships demographic location features.","code":""},{"path":"/reference/generate_population.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Population — generate_population","text":"","code":"generate_population(N = 1000, regions = NULL, capacities = NULL,   attraction_loci = 2, random_regions = 0.1, cost_loci = 2,   size_loci = 2, similarity_metric = \"euclidean\", n_neighbors = 50,   neighbor_range = 0.5, n_races = 6, verbose = FALSE)"},{"path":"/reference/generate_population.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Population — generate_population","text":"N Number initial individuals generate. Final number individuals larger. regions vector region IDs, matrix coordinates, sf object geometries coordinates can derived. specified (capacities specified), regions similar housing units (mix single multi-family locations) generated. capacities vector maximum number households entry regions. attraction_loci Number locations selected centers attractiveness, influence households located. random_regions number 0 1, determines proportion people randomly relocated, case capacity households. cost_loci Number locations selected centers cost, influences initial income associated households. size_loci Number locations selected centers size, influence household sizes. similarity_metric Name metric use calculate nearness neighbors; see lma_simets. n_neighbors Number neighbors used influence new household's initial age race. neighbor_range Minimum similarity people considered neighbors, 0 1 (0 means unrestricted, 1 means region ). n_races Number different race groups sample . verbose Logical; TRUE, show status messages.","code":""},{"path":"/reference/generate_population.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Population — generate_population","text":"list entries params (initial settings), two data.frames: households   individuals","code":""},{"path":"/reference/generate_population.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a Population — generate_population","text":"population generated two steps: First, households generated placed within regions. Placement within regions determined total distance one regions selected attraction loci. coordinates provided, first randomly generated. households placed, household sizes incomes (first member) generated independent spacial loci. Renting status generated based income: 60% chance income mean income, 40% otherwise. Second, individuals generated household. generate individual, first, neighbors searched , based n_neighbors neighbor_range. neighbors summarized: average age income, tabulated race. affect first member household: age first drawn Beta distribution (shapes 1 2 renting 1.5 otherwise, multiplied 80) added 18, adjusted toward random value centered average neighbor age (floored Gaussian standard deviation 1), race sampled (highest result Binomial draw n_races trials proportion neighbors * base rate chance success race group). Neighbors also affect income second member household first member's income neighbor mean income (40,000 given neighbors); case, second member's income drawn Gaussian distribution centered first member's income, standard deviation 10,000. second member's age based first member; floored Gaussian centered first member's age, standard deviation 15 first member's age 40, 5 otherwise, trimmed 18 90. second member's race 70% chance first member, 30% chance selected like first member's. Members second income, age randomly selected uniform distribution 0 first member's age minus 15 (rounded ), race determined either first second member (50% chance). Sex 50% chance 0 1 second member; sex 10% chance first member's, 90% chance opposite.","code":""},{"path":"/reference/generate_population.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a Population — generate_population","text":"","code":"generate_population(2) #> $params #> $params$neighbors #> [1] 50 #>  #> $params$range #> [1] 0.5 #>  #> $params$races_rates #> [1] 0.3428109 0.4445378 0.4981593 0.3600173 0.4777192 0.5000000 #>  #>  #> $households #>   household region head_income size renting #> 1         1      1       73277    1       1 #> 2         2      2      102784    3       1 #>  #> $individuals #>   household person neighbors age sex race income #> 1         1      1         0  52   1    5  73277 #> 2         2      2         0  68   0    4 102784 #> 3         2      3         0  70   1    4      0 #> 4         2      4         0  17   0    4      0 #>"},{"path":"/reference/make_parent_polygons.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Higher-Order Polygons — make_parent_polygons","title":"Make Higher-Order Polygons — make_parent_polygons","text":"Randomly combine given polygons contiguous, larger polygons.","code":""},{"path":"/reference/make_parent_polygons.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Higher-Order Polygons — make_parent_polygons","text":"","code":"make_parent_polygons(polygons, ids = NULL, n = 2, strict_n = TRUE,   n_as_min = FALSE, buffer_dist = 5e-05, min_overlap = NULL,   verbose = TRUE)"},{"path":"/reference/make_parent_polygons.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Higher-Order Polygons — make_parent_polygons","text":"polygons sf object polygons reformed. ids vector IDs length polygons, column name polygons use IDs. n Number polygons aim combine new polygon. strict_n Logical; TRUE (default), n represents number intersecting polygons select, depending availability (minimum 2). FALSE, n represents number nearest polygons (centroid) use calculating box use selecting polygons (minimum 1). n_as_min Logical; TRUE, merge parents fewer n children random neighbor. Otherwise (default), parents may fewer n children. Applies strict_n TRUE. buffer_dist Distance around initial shape set shapes, used define neighboring shapes. Applies strict_n TRUE min_overlap Minimal area overlap potential neighboring shapes buffered target shape, used define neighboring shapes. Applies strict_n TRUE verbose Logical; FALSE, print status messages.","code":""},{"path":"/reference/make_parent_polygons.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Higher-Order Polygons — make_parent_polygons","text":"list entries new (sf object containing new polygons) map (list mapping old new polygon indices).","code":""},{"path":"/reference/redistribute.html","id":null,"dir":"Reference","previous_headings":"","what":"Redistribute Data — redistribute","title":"Redistribute Data — redistribute","text":"Distribute data source frame target frame.","code":""},{"path":"/reference/redistribute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Redistribute Data — redistribute","text":"","code":"redistribute(source, target = NULL, map = list(), source_id = \"GEOID\",   target_id = source_id, weight = NULL, source_variable = NULL,   source_value = NULL, aggregate = NULL, weight_agg_method = \"auto\",   default_value = NA, outFile = NULL, overwrite = FALSE,   make_intersect_map = FALSE, overlaps = \"keep\", use_all = TRUE,   return_geometry = TRUE, return_map = FALSE, verbose = FALSE)"},{"path":"/reference/redistribute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Redistribute Data — redistribute","text":"source matrix-like object want distribute ; usually real complete dataset, often lower resolution / higher level. target matrix-like object want distribute : usually dataset want available, often higher resolution / lower level (disaggregation). Can also single number, representing number initial characters source IDs derive target IDs (useful aggregating nested groups). map list entries named source IDs (aligning IDs), containing vectors associated target IDs (indices IDs). Entries can also numeric vectors IDs names, used weigh relationship. IDs related substrings (first characters target IDs source IDs), map can automatically generated . source target contain sf geometries, map made st_intersects (st_intersects(source, target)). intersects map made, source aggregated target, map entries contain multiple target IDs, entries weighted proportion overlap source area. source_id, target_id Name column source / target, vector containing IDs. source, default first column. target, columns searched one appears relate source IDs, falling back first column. weight Name column, vector containing weights (single value apply cases), apply target disaggregating, source aggregating. Defaults unit weights (weights 1). source_variable, source_value source tall (variables spread across rows rather columns), specifies names columns source containing variable names values conversion. aggregate Logical; specified, determine whether aggregate disaggregate source target. Otherwise, TRUE source observations target observations. weight_agg_method Means aggregating weight, case target IDs contain duplicates. Options \"sum\", \"average\", \"auto\" (default; sum weight integer-like, average otherwise). default_value Value set unmapped target ID. outFile Path CSV file save results. overwrite Logical; TRUE, overwrite existing outFile. make_intersect_map Logical; TRUE, opt calculate intersect-based map rather ID-based map, seem possible. specified FALSE, never calculate intersect-based map. overlaps specified TRUE \"keep\", assign target entities mapped multiple source entities single source entity. value determines entities weight assigned, \"first\" (default), \"last\", \"random\". use_all Logical; TRUE (default), redistribute map weights sum 1. Otherwise, entities may partially weighted. return_geometry Logical; FALSE, set returned data.frame's geometry target, exists. return_map Logical; TRUE, return map, without performing redistribution. Useful want inspect automatically created map, use later call. verbose Logical; TRUE, show status messages.","code":""},{"path":"/reference/redistribute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Redistribute Data — redistribute","text":"data.frame row target_ids (identified first column, id), column variable source.","code":""},{"path":"/reference/redistribute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Redistribute Data — redistribute","text":"","code":"# minimal example source <- c(a = 1, b = 2) target <- 1:5 (redistribute(source, target, verbose = TRUE)) #> ℹ source IDs: 1 #> ℹ target IDs: `target` vector #> ℹ map: all target IDs for single source #> ℹ weights: 1 #> ℹ redistributing 2 variables from 1 source to 5 targets: #> • (numb; 2) a, b #> ℹ disaggregating... #> ✔ done disaggregating [12ms] #>  #>   id   a   b #> 1  1 0.2 0.4 #> 2  2 0.2 0.4 #> 3  3 0.2 0.4 #> 4  4 0.2 0.4 #> 5  5 0.2 0.4  # multi-entity example source <- data.frame(id = c(\"a\", \"b\"), cat = c(\"aaa\", \"bbb\"), num = c(1, 2)) target <- data.frame(   id = sample(paste0(c(\"a\", \"b\"), rep(1:5, 2))),   population = sample.int(1e5, 10) ) (redistribute(source, target, verbose = TRUE)) #> ℹ source IDs: id column of `source` #> ℹ target IDs: id column of `target` #> ℹ map: first 1 character of target IDs #> ℹ weights: 1 #> ℹ redistributing 2 variables from 2 sources to 10 targets: #> • (numb; 1) num #> • (char; 1) cat #> ℹ disaggregating... #> ✔ done disaggregating [7ms] #>  #> ℹ re-converting categorical levels #>    id cat num #> 1  b4 bbb 0.4 #> 2  b3 bbb 0.4 #> 3  a2 aaa 0.2 #> 4  a1 aaa 0.2 #> 5  a5 aaa 0.2 #> 6  a3 aaa 0.2 #> 7  b5 bbb 0.4 #> 8  a4 aaa 0.2 #> 9  b1 bbb 0.4 #> 10 b2 bbb 0.4"}]
